[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "QA Catalogue",
    "section": "",
    "text": "Introduction\nQA catalogue is a set of software packages for bibliographical record quality assessment. It reads MARC or PICA files (in different formats), analyses some quality dimensions, and saves the results into CSV files. These CSV files could be used in different context, we provide a lightweight, web-based user interface for that. Some of the functionalities are available as a web service, so the validation could be built into a cataloguing/quality assessment workflow.\n\n\n\n\n\n\n\n\n\nFigure 1: QA Catalogue’s web user interface\n\n\n\n\nFor more info\n\nmain project page: Metadata Quality Assessment Framework\nValidating 126 million MARC records at DATeCH 2019 paper, slides, thesis chapter\nEmpirical evaluation of library catalogues at SWIB 2019 slides, paper in English, paper in Spanish\nquality assessment of Gent University Library catalogue (a running instance of the dashboard): http://gent.qa-catalogue.eu/\nnew: QA catalogue mailing list https://listserv.gwdg.de/mailman/listinfo/qa-catalogue\n\nIf you would like to play with this project, but you don’t have MARC21 please to download some recordsets mentioned in Appendix I: Where can I get MARC records? of this document.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "quick-start-guide.html",
    "href": "quick-start-guide.html",
    "title": "1  Quick start guide",
    "section": "",
    "text": "1.1 Installation\nSee INSTALL.md for dependencies.",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick start guide</span>"
    ]
  },
  {
    "objectID": "quick-start-guide.html#installation",
    "href": "quick-start-guide.html#installation",
    "title": "1  Quick start guide",
    "section": "",
    "text": "wget https://github.com/pkiraly/metadata-qa-marc/releases/download/v0.6.0/metadata-qa-marc-0.6.0-release.zip\nunzip metadata-qa-marc-0.6.0-release.zip\ncd metadata-qa-marc-0.6.0/",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick start guide</span>"
    ]
  },
  {
    "objectID": "quick-start-guide.html#configuration",
    "href": "quick-start-guide.html#configuration",
    "title": "1  Quick start guide",
    "section": "1.2 Configuration",
    "text": "1.2 Configuration\nEither use the script qa-catalogue or create configuration files:\n\ncp setdir.sh.template setdir.sh\n\nChange the input and output base directories in setdir.sh. Local directories input/ and output/ will be used by default. Files of each catalogue are in a subdirectory of theses base directories:\n\nCreate configuration based on some existing config files:\n\n\ncp catalogues/loc.sh catalogues/[abbreviation-of-your-library].sh\nedit catalogues/[abbreviation-of-your-library].sh according to configuration guide",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick start guide</span>"
    ]
  },
  {
    "objectID": "quick-start-guide.html#with-docker",
    "href": "quick-start-guide.html#with-docker",
    "title": "1  Quick start guide",
    "section": "1.3 With Docker",
    "text": "1.3 With Docker\nA more detailed instruction how to use qa-catalogue with Docker can be found in the wiki\nA Docker image bundling qa-catalogue with all of its dependencies and the web interface qa-catalogue-web is made available:\n\ncontinuously via GitHub as ghcr.io/pkiraly/qa-catalogue\nand for releases via Docker Hub as pkiraly/metadata-qa-marc\n\nTo download, configure and start an image in a new container the file docker-compose.yml is needed in the current directory. It can be configured with the following environment variables:\n\nIMAGE: which Docker image to download and run. By default the latest image from Docker Hub is used (pkiraly/metadata-qa-marc). Alternatives include\n\nIMAGE=ghcr.io/pkiraly/qa-catalogue:main for most recent image from GitHub packages\nIMAGE=metadata-qa-marc if you have locally build the Docker image\n\nCONTAINER: the name of the docker container. Default: metadata-qa-marc.\nINPUT: Base directory to put your bibliographic record files in subdirectories of. Set to ./input by default, so record files are expected to be in input/$NAME.\nOUTPUT: Base directory to put result of qa-catalogue in subdirectory of. Set to ./output by default, so files are put in output/$NAME.\nWEBCONFIG: directory to expose configuration of qa-catalogue-web. Set to ./web-config by default. If using non-default configuration for data analysis (for instance PICA instead of MARC) then you likely need to adjust configuration of the web interface as well. This directory should contain a configuration file configuration.cnf.\nWEBPORT: port to expose the web interface. For instance WEBPORT=9000 will make it available at http://localhost:9000/ instead of http://localhost/.\nSOLRPORT: port to expose Solr to. Default: 8983.\n\nEnvironment variables can be set on command line or be put in local file .env, e.g.:\nWEBPORT=9000 docker compose up -d\nor\ndocker compose --env-file config.env up -d\nWhen the application has been started this way, run analyses with script ./docker/qa-catalogue the same ways as script ./qa-catalogue is called when not using Docker (see usage for details). The following example uses parameters for Gent university library catalogue:\n./docker/qa-catalogue \\\n  --params \"--marcVersion GENT --alephseq\" \\\n  --mask \"rug01.export\" \\\n  --catalogue gent \\\n  all\nNow you can reach the web interface (qa-catalogue-web) at http://localhost:80/ (or at another port as configured with environment variable WEBPORT). To further modify appearance of the interface, create templates in your WEBCONFIG directory and/or create a file configuration.cnf in this directory to extend UI configuration without having to restart the Docker container.\nThis example works under Linux. Windows users should consult the Docker on Windows wiki page. Other useful Docker commands at QA catalogue’s wiki.\nEverything else should work the same way as in other environments, so follow the next sections.",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick start guide</span>"
    ]
  },
  {
    "objectID": "quick-start-guide.html#use",
    "href": "quick-start-guide.html#use",
    "title": "1  Quick start guide",
    "section": "1.4 Use",
    "text": "1.4 Use\ncatalogues/[abbreviation-of-your-library].sh all-analyses\ncatalogues/[abbreviation-of-your-library].sh all-solr\nFor a catalogue with around 1 million record the first command will take 5-10 minutes, the later 1-2 hours.",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quick start guide</span>"
    ]
  },
  {
    "objectID": "build.html",
    "href": "build.html",
    "title": "2  Build from source",
    "section": "",
    "text": "Prerequisites: Java 11 (I use OpenJDK), and Maven 3\nOptional step: clone and build the parent library, metadata-qa-api project:\ngit clone https://github.com/pkiraly/metadata-qa-api.git\ncd metadata-qa-api\nmvn clean install\ncd ..\nMandatory step: clone and build the current qa-catalogue project:\ngit clone https://github.com/pkiraly/qa-catalogue.git\ncd metadata-qa-marc\nmvn clean install",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Build from source</span>"
    ]
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "3  Download",
    "section": "",
    "text": "The released versions of the software is available from Maven Central repository. The stable releases (currently 0.6.0) is available from all Maven repos, while the developer version (*-SNAPSHOT) is available only from the Sonatype Maven repository. What you need to select is the file metadata-qa-marc-0.6.0-jar-with-dependencies.jar.\nBe aware that no automation exists for creating a current developer version as nightly build, so there is a chance that the latest features are not available in this version. If you want to use the latest version, do build it.\nSince the jar file doesn’t contain the helper scripts, you might also consider downloading them from this GitHub repository:\nwget https://raw.githubusercontent.com/pkiraly/qa-catalogue/master/common-script\nwget https://raw.githubusercontent.com/pkiraly/qa-catalogue/master/validator\nwget https://raw.githubusercontent.com/pkiraly/qa-catalogue/master/formatter\nwget https://raw.githubusercontent.com/pkiraly/qa-catalogue/master/tt-completeness\nYou should adjust common-script to point to the jar file you just downloaded.",
    "crumbs": [
      "Installation",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Download</span>"
    ]
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Usage",
    "section": "",
    "text": "Helper scripts\nThe tool comes with some bash helper scripts to run all these with default values. The generic scripts locate in the root directory and library specific configuration like scripts exist in the catalogues directory. You can find predefined scripts for several library catalogues (if you want to run it, first you have to configure it). All these scrips mainly contain configuration, and then it calls the central common-script which contains the functions.\nIf you do not want to",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "usage.html#helper-scripts",
    "href": "usage.html#helper-scripts",
    "title": "Usage",
    "section": "",
    "text": "run\ncatalogues/[your script] [command(s)]\nor\n./qa-catalogue --params=\"[options]\" [command(s)]\nThe following commands are supported:\n\nvalidate – runs validation\ncompleteness – runs completeness analysis\nclassifications – runs classification analysis\nauthorities – runs authorities analysis\ntt-completeness – runs Thomson-Trail completeness analysis\nshelf-ready-completeness – runs shelf-ready completeness analysis\nserial-score – calculates the serial scores\nformat – runs formatting records\nfunctional-analysis – runs functional analysis\npareto – runs pareto analysis\nmarc-history – generates cataloguing history chart\nprepare-solr – prepare Solr index (you should already have Solr running, and index created)\nindex – runs indexing with Solr\nsqlite – import tables to SQLite\nexport-schema-files – export schema files\nall-analyses – run all default analysis tasks\nall-solr – run all indexing tasks\nall – run all tasks\nconfig – show configuration of selected catalogue\n\nYou can find information about these functionalities below this document.\n\n\nconfiguration\n\ncreate the configuration file (setdir.sh)\n\ncp setdir.sh.template setdir.sh\n\nedit the file configuration file. Two lines are important here\n\nBASE_INPUT_DIR=your/path\nBASE_OUTPUT_DIR=your/path\nBASE_LOG_DIR==your/path\n\nBASE_INPUT_DIR is the parent directory where your MARC records exists\nBASE_OUTPUT_DIR is where the analysis results will be stored\nBASE_LOG_DIR is where the analysis logs will be stored\n\n\nedit the library specific file\n\nHere is an example file for analysing Library of Congress’ MARC records\n#!/usr/bin/env bash\n\n. ./setdir.sh\n\nNAME=loc\nMARC_DIR=${BASE_INPUT_DIR}/loc/marc\nMASK=*.mrc\n\n. ./common-script\nThree variables are important here:\n\nNAME is a name for the output directory. The analysis result will land under \\(BASE_OUTPUT_DIR/\\)NAME directory\nMARC_DIR is the location of MARC files. All the files should be in the same directory\nMASK is a file mask, such as *.mrc, *.marc or *.dat.gz. Files ending with .gz are uncompressed automatically.\n\nYou can add here any other parameters this document mentioned at the description of individual command, wrapped in TYPE_PARAMS variable e.g. for the Deutche Nationalbibliothek’s config file, one can find this\nTYPE_PARAMS=\"--marcVersion DNB --marcxml\"\nThis line sets the DNB’s MARC version (to cover fields defined within DNB’s MARC version), and XML as input format.\nThe following table summarizes the configuration variables. The script qa-catalogue can be used to set variables and execute analysis without a library specific configuration file:\n\n\n\n\n\n\n\n\n\nvariable\nqa-catalogue\ndescription\ndefault\n\n\n\n\nANALYSES\n-a/--analyses\nwhich tasks to run with all-analyses\nvalidate, validate_sqlite, completeness, completeness_sqlite, classifications, authorities, tt_completeness, shelf_ready_completeness, serial_score, functional_analysis, pareto, marc_history\n\n\n\n-c/--catalogue\ndisplay name of the catalogue\n$NAME\n\n\nNAME\n-n/--name\nname of the catalogue\nqa-catalogue\n\n\nBASE_INPUT_DIR\n-d/--input\nparent directory of input file directories\n./input\n\n\nINPUT_DIR\n-d/--input-dir\nsubdirectory of input directory to read files from\n\n\n\nBASE_OUTPUT_DIR\n-o/--output\nparent output directory\n./output\n\n\nBASE_LOG_DIR\n-l/--logs\ndirectory of log files\n./logs\n\n\nMASK\n-m/--mask\na file mask which input files to process, e.g. *.mrc\n*\n\n\nTYPE_PARAMS\n-p/--params\nparameters to pass to individual tasks (see below)\n\n\n\nSCHEMA\n-s/--schema\nrecord schema\nMARC21\n\n\nUPDATE\n-u/--update\noptional date of input files\n\n\n\nVERSION\n-v/--version\noptional version number/date of the catalogue to compare changes\n\n\n\nWEB_CONFIG\n-w/--web-config\nupdate the specified configuration file of qa-catalogue-web\n\n\n\n\n-f/--env-file\nconfiguration file to load environment variables from (default: .env)",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "usage.html#detailed-instructions",
    "href": "usage.html#detailed-instructions",
    "title": "Usage",
    "section": "Detailed instructions",
    "text": "Detailed instructions\nWe will use the same jar file in every command, so we save its path into a variable.\nexport JAR=target/metadata-qa-marc-0.7.0-jar-with-dependencies.jar\n\nGeneral parameters\nMost of the analyses uses the following general parameters\n\n--schemaType &lt;type&gt; metadata schema type. The supported types are:\n\nMARC21\nPICA\nUNIMARC (assessment of UNIMARC records are not yet supported, this parameter value is only reserved for future usage)\n\n-m &lt;version&gt;, --marcVersion &lt;version&gt; specifies a MARC version. Currently, the supported versions are:\n\nMARC21, Library of Congress MARC21\nDNB, the Deuthche Nationalbibliothek’s MARC version\nOCLC, the OCLCMARC\nGENT, fields available in the catalog of Gent University (Belgium)\nSZTE, fields available in the catalog of Szegedi Tudományegyetem (Hungary)\nFENNICA, fields available in the Fennica catalog of Finnish National Library\nNKCR, fields available at the National Library of the Czech Republic\nBL, fields available at the British Library\nMARC21NO, fields available at the MARC21 profile for Norwegian public libraries\nUVA, fields available at the University of Amsterdam Library\nB3KAT, fields available at the B3Kat union catalogue of Bibliotheksverbundes Bayern (BVB) and Kooperativen Bibliotheksverbundes Berlin-Brandenburg (KOBV)\nKBR, fields available at KBR, the national library of Belgium\nZB, fields available at Zentralbibliothek Zürich\nOGYK, fields available at Országygyűlési Könyvtár, Budapest\n\n-n, --nolog do not display log messages\nparameters to limit the validation:\n\n-i [record ID], --id [record ID] validates only a single record having the specifies identifier (the content of 001)\n-l [number], --limit [number] validates only given number of records\n-o [number], --offset [number] starts validation at the given Nth record\n-z [list of tags], --ignorableFields [list of tags] do NOT validate the selected fields. The list should contain the tags separated by commas (,), e.g. --ignorableFields A02,AQN\n-v [selector], --ignorableRecords [selector] do NOT validate the records which match the condition denoted by the selector. The selector is a test MARCspec string e.g. --ignorableRecords STA$a=SUPPRESSED. It ignores the records which has STA field with an a subfield with the value SUPPRESSED.\n\n-d [record type], --defaultRecordType [record type] the default record type to be used if the record’s type is undetectable. The record type is calculated from the combination of Leader/06 (Type of record) and Leader/07 (bibliographic level), however sometimes the combination doesn’t fit to the standard. In this case the tool will use the given record type. Possible values of the record type argument:\n\nBOOKS\nCONTINUING_RESOURCES\nMUSIC\nMAPS\nVISUAL_MATERIALS\nCOMPUTER_FILES\nMIXED_MATERIALS\n\nparameters to fix known issues before any analyses:\n\n-q, --fixAlephseq sometimes ALEPH export contains ‘^’ characters instead of spaces in control fields (006, 007, 008). This flag replaces them with spaces before the validation. It might occur in any input format.\n-a, --fixAlma sometimes Alma export contains ‘#’ characters instead of spaces in control fields (006, 007, 008). This flag replaces them with spaces before the validation. It might occur in any input format.\n-b, --fixKbr KBR’s export contains ‘#’ characters instead spaces in control fields (006, 007, 008). This flag replaces them with spaces before the validation. It might occur in any input format.\n\n-f &lt;format&gt;, --marcFormat &lt;format&gt; The input format. Possible values are\n\nISO: Binary (ISO 2709)\nXML: MARCXML (shortcuts: -x, --marcxml)\nALEPHSEQ: Alephseq (shortcuts: -p, --alephseq)\nLINE_SEPARATED: Line separated binary MARC where each line contains one record) (shortcuts: -y, --lineSeparated)\nMARC_LINE: MARC Line is a line-separated format i.e. it is a text file, where each line is a distinct field, the same way as MARC records are usually displayed in the MARC21 standard documentation.\nMARCMAKER: MARCMaker format\nPICA_PLAIN: PICA plain (https://format.gbv.de/pica/plain) is a serialization format, that contains each fields in distinct row.\nPICA_NORMALIZED: normalized PICA (https://format.gbv.de/pica/normalized) is a serialization format where each line is a separate record (by bytecode 0A). Fields are terminated by bytecode 1E, and subfields are introduced by bytecode 1F.\n\n-t &lt;directory&gt;, --outputDir &lt;directory&gt; specifies the output directory where the files will be created\n-r, --trimId remove spaces from the end of record IDs in the output files (some library system add padding spaces around field value 001 in exported files)\n-g &lt;encoding&gt;, --defaultEncoding &lt;encoding&gt; specify a default encoding of the records. Possible values:\n\nISO-8859-1 or ISO8859_1 or ISO_8859_1\nUTF8 or UTF-8\nMARC-8 or MARC8\n\n-s &lt;datasource&gt;, --dataSource &lt;datasource&gt; specify the type of data source. Possible values:\n\nFILE: reading from file\nSTREAM: reading from a Java data stream. It is not usable if you use the tool from the command line, only if you use it with its API.\n\n-c &lt;configuration&gt;, --allowableRecords &lt;configuration&gt; if set, criteria which allows analysis of records. If the record does not met the criteria, it will be excluded. An individual criterium should be formed as a MarcSpec (for MARC21 records) or PicaFilter (for PICA records). Multiple criteria might be concatenated with logical operations: && for AND, || for OR and ! for not. One can use parentheses to group logical expressions. An example: '002@.0 !~ \"^L\" && 002@.0 !~ \"^..[iktN]\" && (002@.0 !~ \"^.v\" || 021A.a?)'. Since the criteria might form a complex phase containing spaces, the passing of which is problematic among multiple scripts, one can apply Base64 encoding. In this case add base64: prefix to the parameters, such as base64:\"$(echo '002@.0 !~ \"^L\" && 002@.0 !~ \"^..[iktN]\" && (002@.0 !~ \"^.v\" || 021A.a?)' | base64 -w 0).\n-1 &lt;type&gt;, --alephseqLineType &lt;type&gt;, true, “Alephseq line type. The type could be\n\nWITH_L: the records’ AlephSeq lines contain an L string (e.g. 000000002 008   L 780804s1977^^^^enk||||||b||||001^0|eng||)\nWITHOUT_L: the records’ AlephSeq lines do not contai an L string (e.g. 000000002 008   780804s1977^^^^enk||||||b||||001^0|eng||)\n\nPICA related parameters\n\n-2 &lt;path&gt;, --picaIdField &lt;path&gt; the record identifier\n-u &lt;char&gt;, --picaSubfieldSeparator &lt;char&gt; the PICA subfield separator. subfield of PICA records. Default is 003@$0. Default is $.\n-j &lt;file&gt;, --picaSchemaFile &lt;file&gt; an Avram schema file, which describes the structure of PICA records\n-k &lt;path&gt;, --picaRecordType &lt;path&gt; The PICA subfield which stores the record type information. Default is 002@$0.\n\nParameters for grouping analyses\n\n-e &lt;path&gt;, --groupBy &lt;path&gt; group the results by the value of this data element (e.g. the ILN of libraries holding the item). An example: --groupBy 001@$0 where 001@$0 is the subfield containing the comma separated list of library ILN codes.\n-3 &lt;file&gt;, --groupListFile &lt;file&gt; the file which contains a list of ILN codes\n\n\nThe last argument of the commands are a list of files. It might contain any wildcard the operating system supports (’*‘,’?’, etc.).",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "validate.html",
    "href": "validate.html",
    "title": "4  Validating MARC records",
    "section": "",
    "text": "It validates each records against the MARC21 standard, including those local defined field, which are selected by the MARC version parameter.\nThe issues are classified into the following categories: record, control field, data field, indicator, subfield and their subtypes.\nThere is an uncertainty in the issue detection. Almost all library catalogues have fields, which are not part of the MARC standard, neither that of their documentation about the locally defined fields (these documents are rarely available publicly, and even if they are available sometimes they do not cover all fields). So if the tool meets a field which are undefined, it is impossible to decide whether it is valid or invalid in a particular context. So in some places the tool reflects this uncertainty and provides two calculations, one which handles these fields as error, and another which handles these as valid fields.\nThe tool detects the following issues:\n\n\n\n\n\n\n\nmachine name\nexplanation\n\n\n\n\nrecord level issues\n\n\n\nundetectableType\nthe document type is not detectable\n\n\ninvalidLinkage\nthe linkage in field 880 is invalid\n\n\nambiguousLinkage\nthe linkage in field 880 is ambiguous\n\n\ncontrol field position issues\n\n\n\nobsoleteControlPosition\nthe code in the position is obsolete (it was valid in a previous version of MARC, but it is not valid now)\n\n\ncontrolValueContainsInvalidCode\nthe code in the position is invalid\n\n\ninvalidValue\nthe position value is invalid\n\n\ndata field issues\n\n\n\nmissingSubfield\nmissing reference subfield (880$6)\n\n\nnonrepeatableField\nrepetition of a non-repeatable field\n\n\nundefinedField\nthe field is not defined in the specified MARC version(s)\n\n\nindicator issues\n\n\n\nobsoleteIndicator\nthe indicator value is obsolete (it was valid in a previous version of MARC, but not in the current version)\n\n\nnonEmptyIndicator\nindicator that should be empty is non-empty\n\n\ninvalidValue\nthe indicator value is invalid\n\n\nsubfield issues\n\n\n\nundefinedSubfield\nthe subfield is undefined in the specified MARC version(s)\n\n\ninvalidLength\nthe length of the value is invalid\n\n\ninvalidReference\nthe reference to the classification vocabulary is invalid\n\n\npatternMismatch\ncontent does not match the patterns specified by the standard\n\n\nnonrepeatableSubfield\nrepetition of a non-repeatable subfield\n\n\ninvalidISBN\ninvalid ISBN value\n\n\ninvalidISSN\ninvalid ISSN value\n\n\nunparsableContent\nthe value of the subfield is not well-formed according to its specification\n\n\nnullCode\nnull subfield code\n\n\ninvalidValue\ninvalid subfield value\n\n\n\nUsage:\njava -cp $JAR de.gwdg.metadataqa.marc.cli.Validator [options] &lt;file&gt;\nor with a bash script\n./validator [options] &lt;file&gt;\nor\ncatalogues/&lt;catalogue&gt;.sh validate\nor\n./qa-catalogue --params=\"[options]\" validate\noptions:\n\ngeneral parameters\ngranularity of the report\n\n-S, --summary: creating a summary report instead of record level reports\n-H, --details: provides record level details of the issues\n\noutput parameters:\n\n-G &lt;file&gt;, --summaryFileName &lt;file&gt;: the name of summary report the program produces. The file provides a summary of issues, such as the number of instance and number of records having the particular issue.\n-F &lt;file&gt;, --detailsFileName &lt;file&gt;: the name of report the program produces. Default is validation-report.txt. If you use “stdout”, it won’t create file, but put results into the standard output.\n-R &lt;format&gt;, --format &lt;format&gt;: format specification of the output. Possible values:\n\ntext (default),\ntab-separated or tsv,\ncomma-separated or csv\n\n\n-W, --emptyLargeCollectors: the output files are created during the process and not only at the end of it. It helps in memory management if the input is large, and it has lots of errors, on the other hand the output file will be segmented, which should be handled after the process.\n-T, --collectAllErrors: collect all errors (useful only for validating small number of records). Default is turned off.\n-I &lt;types&gt;, --ignorableIssueTypes &lt;types&gt;: comma separated list of issue types not to collect. The valid values are (for details see the issue types table):\n\nundetectableType: undetectable type\ninvalidLinkage: invalid linkage\nambiguousLinkage: ambiguous linkage\nobsoleteControlPosition: obsolete code\ncontrolValueContainsInvalidCode: invalid code\ninvalidValue: invalid value\nmissingSubfield: missing reference subfield (880$6)\nnonrepeatableField: repetition of non-repeatable field\nundefinedField: undefined field\nobsoleteIndicator: obsolete value\nnonEmptyIndicator: non-empty indicator\ninvalidValue: invalid value\nundefinedSubfield: undefined subfield\ninvalidLength: invalid length\ninvalidReference: invalid classification reference\npatternMismatch: content does not match any patterns\nnonrepeatableSubfield: repetition of non-repeatable subfield\ninvalidISBN: invalid ISBN\ninvalidISSN: invalid ISSN\nunparsableContent: content is not well-formatted\nnullCode: null subfield code\ninvalidValue: invalid value\n\n\nOutputs: * count.csv: the count of bibliographic records in the source dataset\ntotal\n1192536\n\nissue-by-category.csv: the counts of issues by categories. Columns:\n\nid the identifier of error category\ncategory the name of the category\ninstances the number of instances of errors within the category (one record might have multiple instances of the same error)\nrecords the number of records having at least one of the errors within the category\n\n\nid,category,instances,records\n2,control field,994241,313960\n3,data field,12,12\n4,indicator,5990,5041\n5,subfield,571,555\n\nissue-by-type.csv: the count of issues by types (subcategories).\n\nid,categoryId,category,type,instances,records\n5,2,control field,\"invalid code\",951,541\n6,2,control field,\"invalid value\",993290,313733\n8,3,data field,\"repetition of non-repeatable field\",12,12\n10,4,indicator,\"obsolete value\",1,1\n11,4,indicator,\"non-empty indicator\",33,32\n12,4,indicator,\"invalid value\",5956,5018\n13,5,subfield,\"undefined subfield\",48,48\n14,5,subfield,\"invalid length\",2,2\n15,5,subfield,\"invalid classification reference\",2,2\n16,5,subfield,\"content does not match any patterns\",286,275\n17,5,subfield,\"repetition of non-repeatable subfield\",123,120\n18,5,subfield,\"invalid ISBN\",5,3\n19,5,subfield,\"invalid ISSN\",105,105\n\nissue-summary.csv: details of individual issues including basic statistics\n\nid,MarcPath,categoryId,typeId,type,message,url,instances,records\n53,008/33-34 (008map33),2,5,invalid code,'b' in 'b ',https://www.loc.gov/marc/bibliographic/bd008p.html,1,1\n70,008/00-05 (008all00),2,5,invalid code,Invalid content: '2023  '. Text '2023  ' could not be parsed at index 4,https://www.loc.gov/marc/bibliographic/bd008a.html,1,1\n28,008/22-23 (008map22),2,6,invalid value,| ,https://www.loc.gov/marc/bibliographic/bd008p.html,12,12\n19,008/31 (008book31),2,6,invalid value, ,https://www.loc.gov/marc/bibliographic/bd008b.html,1,1\n17,008/29 (008book29),2,6,invalid value, ,https://www.loc.gov/marc/bibliographic/bd008b.html,1,1\n\nissue-details.csv: list of issues by record identifiers. It has two columns, the record identifier, and a complex string, which contains the number of occurrences of each individual issue concatenated by semicolon.\n\nrecordId,errors\n99117335059205508,1:2;2:1;3:1\n99117335059305508,1:1\n99117335059405508,2:2\n99117335059505508,3:1\n1:2;2:1;3:1 means that 3 different types of issues are occurred in the record, the firs issue which has issue ID 1 occurred twice, issue ID 2 which occurred once and issue ID 3, which occurred once. The issue IDs can be resolved from the issue-summary.csv file’s firs column.\n\nissue-details-normalized.csv: the normalized version of the previous file\n\nid,errorId,instances\n99117335059205508,1,2\n99117335059205508,2,1\n99117335059205508,3,1\n99117335059305508,1,1\n99117335059405508,2,2\n99117335059505508,3,1\n\nissue-total.csv: the number of issue free records, and number of record having issues\n\ntype,instances,records\n0,0,251\n1,1711,848\n2,413,275\nwhere types are - 0: records without errors - 1: records with any kinds of errors - 2: records with errors excluding invalid field errors\n\nissue-collector.csv: non normalized file of record ids per issues. This is the “inverse” of issue-details.csv, it tells you in which records a particular issue occurred.\n\nerrorId,recordIds\n1,99117329355705508;99117328948305508;99117334968905508;99117335067705508;99117335176005508;...\n\nvalidation.params.json: the list of the actual parameters during the running of the validation\n\nAn example with parameters used for analysing a PICA dataset. When the input is a complex expression it is displayed here in a parsed format. It also contains some metadata such as the versions of MQFA API and QA catalogue.\n{\n  \"args\":[\"/path/to/input.dat\"],\n  \"marcVersion\":\"MARC21\",\n  \"marcFormat\":\"PICA_NORMALIZED\",\n  \"dataSource\":\"FILE\",\n  \"limit\":-1,\n  \"offset\":-1,\n  \"id\":null,\n  \"defaultRecordType\":\"BOOKS\",\n  \"alephseq\":false,\n  \"marcxml\":false,\n  \"lineSeparated\":false,\n  \"trimId\":true,\n  \"outputDir\":\"/path/to/_output/k10plus_pica\",\n  \"recordIgnorator\":{\n    \"criteria\":[],\n    \"booleanCriteria\":null,\n    \"empty\":true\n  },\n  \"recordFilter\":{\n    \"criteria\":[],\n    \"booleanCriteria\":{\n      \"op\":\"AND\",\n      \"children\":[\n        {\n          \"op\":null,\n          \"children\":[],\n          \"value\":{\n            \"path\":{\n              \"path\":\"002@.0\",\n              \"tag\":\"002@\",\n              \"xtag\":null,\n              \"occurrence\":null,\n              \"subfields\":{\"type\":\"SINGLE\",\"input\":\"0\",\"codes\":[\"0\"]},\n              \"subfieldCodes\":[\"0\"]\n            },\n            \"operator\":\"NOT_MATCH\",\n            \"value\":\"^L\"\n          }\n        },\n        {\"op\":null,\"children\":[],\"value\":{\"path\":{\"path\":\"002@.0\",\"tag\":\"002@\",\"xtag\":null,\"occurrence\":null,\"subfields\":{\"type\":\"SINGLE\",\"input\":\"0\",\"codes\":[\"0\"]},\"subfieldCodes\":[\"0\"]},\"operator\":\"NOT_MATCH\",\"value\":\"^..[iktN]\"}},\n        {\"op\":\"OR\",\"children\":[{\"op\":null,\"children\":[],\"value\":{\"path\":{\"path\":\"002@.0\",\"tag\":\"002@\",\"xtag\":null,\"occurrence\":null,\"subfields\":{\"type\":\"SINGLE\",\"input\":\"0\",\"codes\":[\"0\"]},\"subfieldCodes\":[\"0\"]},\"operator\":\"NOT_MATCH\",\"value\":\"^.v\"}},{\"op\":null,\"children\":[],\"value\":{\"path\":{\"path\":\"021A.a\",\"tag\":\"021A\",\"xtag\":null,\"occurrence\":null,\"subfields\":{\"type\":\"SINGLE\",\"input\":\"a\",\"codes\":[\"a\"]},\"subfieldCodes\":[\"a\"]},\"operator\":\"EXIST\",\"value\":null}}],\"value\":null}\n      ],\n      \"value\":null\n    },\n    \"empty\":false\n  },\n  \"ignorableFields\":{\n    \"fields\":[\"001@\",\"001E\",\"001L\",\"001U\",\"001U\",\"001X\",\"001X\",\"002V\",\"003C\",\"003G\",\"003Z\",\"008G\",\"017N\",\"020F\",\"027D\",\"031B\",\"037I\",\"039V\",\"042@\",\"046G\",\"046T\",\"101@\",\"101E\",\"101U\",\"102D\",\"201E\",\"201U\",\"202D\"],\n    \"empty\":false\n  },\n  \"stream\":null,\n  \"defaultEncoding\":null,\n  \"alephseqLineType\":null,\n  \"picaIdField\":\"003@$0\",\n  \"picaSubfieldSeparator\":\"$\",\n  \"picaSchemaFile\":null,\n  \"picaRecordTypeField\":\"002@$0\",\n  \"schemaType\":\"PICA\",\n  \"groupBy\":null,\n  \"detailsFileName\":\"issue-details.csv\",\n  \"summaryFileName\":\"issue-summary.csv\",\n  \"format\":\"COMMA_SEPARATED\",\n  \"ignorableIssueTypes\":[\"FIELD_UNDEFINED\"],\n  \"pica\":true,\n  \"replacementInControlFields\":null,\n  \"marc21\":false,\n  \"mqaf.version\":\"0.9.2\",\n  \"qa-catalogue.version\":\"0.7.0-SNAPSHOT\"\n}\n\nid-groupid.csv: the pairs of record identifiers - group identifiers.\n\nid,groupId\n010000011,0\n010000011,77\n010000011,2035\n010000011,70\n010000011,20\nCurrently, validation detects the following errors:\nLeader specific errors:\n\nLeader/[position] has an invalid value: ‘[value]’ (e.g. Leader/19 (leader19) has an invalid value: '4')\n\nControl field specific errors:\n\n006/[position] ([name]) contains an invalid code: ‘[code]’ in ‘[value]’ (e.g. 006/01-05 (tag006book01) contains an invalid code: 'n' in '  n ')\n006/[position] ([name]) has an invalid value: ‘[value]’ (e.g. 006/13 (tag006book13) has an invalid value: ' ')\n007/[position] ([name]) contains an invalid code: ‘[code]’ in ‘[value]’\n007/[position] ([name]) has an invalid value: ‘[value]’ (e.g. 007/01 (tag007microform01) has an invalid value: ' ')\n008/[position] ([name]) contains an invalid code: ‘[code]’ in ‘[value]’ (e.g. 008/18-22 (tag008book18) contains an invalid code: 'u' in 'u   ')\n008/[position] ([name]) has an invalid value: ‘[value]’ (e.g. 008/06 (tag008all06) has an invalid value: ' ')\n\nData field specific errors\n\nUnhandled tag(s): [tags] (e.g. Unhandled tag: 265)\n[tag] is not repeatable, however there are [number] instances\n[tag] has invalid subfield(s): [subfield codes] (e.g. 110 has invalid subfield: s)\n[tag]\\([indicator] has invalid code: '[code]' (e.g. `110\\)ind1 has invalid code: ‘2’`)\n[tag]\\([indicator] should be empty, it has '[code]' (e.g. `110\\)ind2 should be empty, it has ‘0’`)\n[tag]\\([subfield code] is not repeatable, however there are [number] instances (e.g. `072\\)a is not repeatable, however there are 2 instances`)\n[tag]\\([subfield code] has an invalid value: [value] (e.g. `046\\)a has an invalid value: ‘fb—–’`)\n\nErrors of specific fields:\n\n045\\(a error in '[value]': length is not 4 char (e.g. `045\\)a error in ‘2209668’: length is not 4 char`)\n045$a error in ‘[value]’: ‘[part]’ does not match any patterns\n880 should have subfield $a\n880 refers to field [tag], which is not defined (e.g. 880 refers to field 590, which is not defined)\n\nAn example:\nError in '   00000034 ': \n  110$ind1 has invalid code: '2'\nError in '   00000056 ': \n  110$ind1 has invalid code: '2'\nError in '   00000057 ': \n  082$ind1 has invalid code: ' '\nError in '   00000086 ': \n  110$ind1 has invalid code: '2'\nError in '   00000119 ': \n  700$ind1 has invalid code: '2'\nError in '   00000234 ': \n  082$ind1 has invalid code: ' '\nErrors in '   00000294 ': \n  050$ind2 has invalid code: ' '\n  260$ind1 has invalid code: '0'\n  710$ind2 has invalid code: '0'\n  710$ind2 has invalid code: '0'\n  710$ind2 has invalid code: '0'\n  740$ind2 has invalid code: '1'\nError in '   00000322 ': \n  110$ind1 has invalid code: '2'\nError in '   00000328 ': \n  082$ind1 has invalid code: ' '\nError in '   00000374 ': \n  082$ind1 has invalid code: ' '\nError in '   00000395 ': \n  082$ind1 has invalid code: ' '\nError in '   00000514 ': \n  082$ind1 has invalid code: ' '\nErrors in '   00000547 ': \n  100$ind2 should be empty, it has '0'\n  260$ind1 has invalid code: '0'\nErrors in '   00000571 ': \n  050$ind2 has invalid code: ' '\n  100$ind2 should be empty, it has '0'\n  260$ind1 has invalid code: '0'\n...\n\n\n4.0.0.1 post processing validation result (validate-sqlite)\nUsage:\ncatalogues/&lt;catalogue&gt;.sh validate-sqlite\nor\n./qa-catalogue --params=\"[options]\" validate-sqlite\nor\n./common-script [options] validate-sqlite\n[options] are the same as for validation\n\n4.0.0.1.1 Catalogue for a single library\nIf the data is not grouped by libraries (no --groupBy &lt;path&gt; parameter), it creates the following SQLite3 database structure and import some of the CSV files into it:\nissue_summary table for the issue-summary.csv:\nIt represents a particular type of error\nid         INTEGER,  -- identifier of the error\nMarcPath   TEXT,     -- the location of the error in the bibliographic record\ncategoryId INTEGER,  -- the identifier of the category of the error\ntypeId     INTEGER,  -- the identifier of the type of the error\ntype       TEXT,     -- the description of the type\nmessage    TEXT,     -- extra contextual information \nurl        TEXT,     -- the url of the definition of the data element\ninstances  INTEGER,  -- the number of instances this error occured\nrecords    INTEGER   -- the number of records this error occured in\nissue_details table for the issue-details.csv:\nEach row represents how many instances of an error occur in a particular bibliographic record\nid         TEXT,    -- the record identifier\nerrorId    INTEGER, -- the error identifier (-&gt; issue_summary.id)\ninstances  INTEGER  -- the number of instances of an error in the record\n\n\n4.0.0.1.2 Union catalogue for multiple libraries\nIf the dataset is a union catalogue, and the record contains a subfield for the libraries holding the item (there is --groupBy &lt;path&gt; parameter), it creates the following SQLite3 database structure and import some of the CSV files into it:\nissue_summary table for the issue-summary.csv (it is similar to the other issue_summary table, but it has an extra groupId column)\ngroupId    INTEGER,\nid         INTEGER,\nMarcPath   TEXT,\ncategoryId INTEGER,\ntypeId     INTEGER,\ntype       TEXT,\nmessage    TEXT,\nurl        TEXT,\ninstances  INTEGER,\nrecords    INTEGER\nissue_details table (same as the other issue_details table)\nid         TEXT,\nerrorId    INTEGER,\ninstances  INTEGER\nid_groupid table for id-groupid.csv:\nid         TEXT,\ngroupId    INTEGER\nissue_group_types table contains statistics for the error types per groups.\ngroupId    INTEGER,\ntypeId     INTEGER,\nrecords    INTEGER,\ninstances  INTEGER\nissue_group_categories table contains statistics for the error categories per groups\ngroupId    INTEGER,\ncategoryId INTEGER,\nrecords    INTEGER,\ninstances  INTEGER\nissue_group_paths table contains statistics for the error types per paths per groups\ngroupId    INTEGER,\ntypeId     INTEGER,\npath       TEXT,\nrecords    INTEGER,\ninstances  INTEGER\nFor union catalogues it also creates an extra Solr index with the suffix _validation. It contains one Solr document for each bibliographic record with three fields: the record identifier, the list of group identifiers and the list of error identifiers (if any). This Solr index is needed for populating the issue_group_types, issue_group_categories and issue_group_paths tables. This index will be ingested into the main Solr index.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Validating MARC records</span>"
    ]
  },
  {
    "objectID": "formatter.html",
    "href": "formatter.html",
    "title": "5  Display one MARC record, or extract data elements from MARC records",
    "section": "",
    "text": "java -cp $JAR de.gwdg.metadataqa.marc.cli.Formatter [options] &lt;file&gt;\nor with a bash script\n./formatter [options] &lt;file&gt;\noptions:\n\ngeneral parameters\n-f, --format: the MARC output format\n\nif not set, the output format follows the examples in the MARC21 documentation (see the example below)\nxml: the output will be MARCXML\n\n-c &lt;number&gt;, -countNr &lt;number&gt;: count number of the record (e.g. 1 means the first record)\n-s [path=query], -search [path=query]: print records matching the query. The query part is the content of the element. The path should be one of the following types:\n\ncontrol field tag (e.g. 001, 002, 003)\ncontrol field position (e.g. Leader/0, 008/1-2)\ndata field (655\\$2, 655\\$ind1)\nnamed control field position (tag006book01)\n\n-l &lt;selector&gt;, --selector &lt;selector&gt;: one or more MarcSpec or PICA Filter selectors, separated by ‘;’ (semicolon) character\n-w, --withId: the generated CSV should contain record ID as first field (default is turned off)\n-p &lt;separator&gt;, --separator &lt;separator&gt;: separator between the parts (default: TAB)\n-e &lt;file&gt;, --fileName &lt;file&gt;: the name of report the program produces (default: extracted.csv)\n-A &lt;identifiers&gt;, --ids &lt;identifiers&gt;: a comma separated list of record identifiers\n\nThe output of displaying a single MARC record is something like this one:\nLEADER 01697pam a2200433 c 4500\n001 1023012219\n003 DE-101\n005 20160912065830.0\n007 tu\n008 120604s2012    gw ||||| |||| 00||||ger  \n015   $a14,B04$z12,N24$2dnb\n016 7 $2DE-101$a1023012219\n020   $a9783860124352$cPp. : EUR 19.50 (DE), EUR 20.10 (AT)$9978-3-86012-435-2\n024 3 $a9783860124352\n035   $a(DE-599)DNB1023012219\n035   $a(OCoLC)864553265\n035   $a(OCoLC)864553328\n040   $a1145$bger$cDE-101$d1140\n041   $ager\n044   $cXA-DE-SN\n082 04$81\\u$a622.0943216$qDE-101$222/ger\n083 7 $a620$a660$qDE-101$222sdnb\n084   $a620$a660$qDE-101$2sdnb\n085   $81\\u$b622\n085   $81\\u$z2$s43216\n090   $ab\n110 1 $0(DE-588)4665669-8$0http://d-nb.info/gnd/4665669-8$0(DE-101)963486896$aHalsbrücke$4aut\n245 00$aHalsbrücke$bzur Geschichte von Gemeinde, Bergbau und Hütten$chrsg. von der Gemeinde Halsbrücke anlässlich des Jubliäums \"400 Jahre Hüttenstandort Halsbrücke\". [Hrsg.: Ulrich Thiel]\n264  1$a[Freiberg]$b[Techn. Univ. Bergakad.]$c2012\n300   $a151 S.$bIll., Kt.$c31 cm, 1000 g\n653   $a(Produktform)Hardback\n653   $aGemeinde Halsbrücke\n653   $aHüttengeschichte\n653   $aFreiberger Bergbau\n653   $a(VLB-WN)1943: Hardcover, Softcover / Sachbücher/Geschichte/Regionalgeschichte, Ländergeschichte\n700 1 $0(DE-588)1113208554$0http://d-nb.info/gnd/1113208554$0(DE-101)1113208554$aThiel, Ulrich$d1955-$4edt$eHrsg.\n850   $aDE-101a$aDE-101b\n856 42$mB:DE-101$qapplication/pdf$uhttp://d-nb.info/1023012219/04$3Inhaltsverzeichnis\n925 r $arb\nAn example for extracting values:\n./formatter --selector \"008~7-10;008~0-5\" \\\n            --defaultRecordType BOOKS \\\n            --separator \",\" \\\n            --outputDir ${OUTPUT_DIR} \\\n            --fileName marc-history.csv \\\n             ${MARC_DIR}/*.mrc\nIt will put the output into ${OUTPUT_DIR}/marc-history.csv.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Display one MARC record, or extract data elements from MARC records</span>"
    ]
  },
  {
    "objectID": "completeness.html",
    "href": "completeness.html",
    "title": "6  Completeness",
    "section": "",
    "text": "6.1 Output files:\nCounts basic statistics about the data elements available in the catalogue.\nUsage:\nor with a bash script\nor\nor\noptions:",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Completeness</span>"
    ]
  },
  {
    "objectID": "completeness.html#output-files",
    "href": "completeness.html#output-files",
    "title": "6  Completeness",
    "section": "",
    "text": "6.1.1 marc-elements.csv\nis list of MARC elements (field$subfield) and their occurrences in two ways as number or records, and number of instances. The columns in the file are:\n\ndocumenttype: the document types found in the dataset. There is an extra document type: all representing all records\npath: the notation of the data element\npackageid and package: each path belongs to one package, such as Control Fields, and each package has an internal identifier.\ntag: the label of tag\nsubfield: the label of subfield\nnumber-of-record: means how many records they are available,\nnumber-of-instances: means how many instances are there in total (some records might contain more than one instances, while others don’t have them at all)\nmin, max, mean, stddev the minimum, maximum, mean and standard deviation of the number of instances per record (as floating point numbers)\nhistogram: the histogram of the instances (1=1; 2=1 means: a single instance is available in one record, two instances are available in one record)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndocumenttype\npath\npackageid\npackage\ntag\nsubfield\nnumber-of-record\nnumber-of-instances\nmin\nmax\nmean\nstddev\nhistogram\n\n\n\n\nall\nleader23\n0\nControl Fields\nLeader\nUndefined\n1099\n1099\n1\n1\n1.0\n0.0\n1=1099\n\n\nall\nleader22\n0\nControl Fields\nLeader\nLength of the implementation-defined portion\n1099\n1099\n1\n1\n1.0\n0.0\n1=1099\n\n\nall\nleader21\n0\nControl Fields\nLeader\nLength of the starting-character-position portion\n1099\n1099\n1\n1\n1.0\n0.0\n1=1099\n\n\nall\n110$a\n2\nMain Entry\nMain Entry - Corporate Name\nCorporate name or jurisdiction name as entry element\n4\n4\n1\n1\n1.0\n0.0\n1=4\n\n\nall\n340$b\n5\nPhysical Description\nPhysical Medium\nDimensions\n2\n3\n1\n2\n1.5\n0.3535533905932738\n1=1; 2=1\n\n\nall\n363$a\n5\nPhysical Description\nNormalized Date and Sequential Designation\nFirst level of enumeration\n1\n1\n1\n1\n1.0\n0.0\n1=1\n\n\nall\n340$a\n5\nPhysical Description\nPhysical Medium\nMaterial base and configuration\n2\n3\n1\n2\n1.5\n0.3535533905932738\n1=1; 2=1\n\n\n\n\n\n6.1.2 packages.csv\nThe completeness of packages (packages are groups of tags)\nIts columns:\n\ndocumenttype: the document type of the record\npackageid: the identifier of the package\nname: name of the package\nlabel: label of the package\niscoretag: does the package belong to the Library of Congress MARC standard\ncount: the number of records having at least one data element from this package\n\n\n\n\n\n\n\n\n\n\n\n\ndocumenttype\npackageid\nname\nlabel\niscoretag\ncount\n\n\n\n\nall\n1\n01X-09X\nNumbers and Code\ntrue\n1099\n\n\nall\n2\n1XX\nMain Entry\ntrue\n816\n\n\nall\n6\n4XX\nSeries Statement\ntrue\n358\n\n\nall\n5\n3XX\nPhysical Description\ntrue\n715\n\n\nall\n8\n6XX\nSubject Access\ntrue\n514\n\n\nall\n4\n25X-28X\nEdition, Imprint\ntrue\n1096\n\n\nall\n7\n5XX\nNote\ntrue\n354\n\n\nall\n0\n00X\nControl Fields\ntrue\n1099\n\n\nall\n99\nunknown\nunknown origin\nfalse\n778\n\n\n\n\n\n6.1.3 libraries.csv\nLists the content of the 852$a (it is useful only if the catalog is an aggregated catalog). Its columns are:\n\nlibrary: the code of a library\ncount: the number of records having a particular library code\n\n\n\n\n\n\n\n\nlibrary\ncount\n\n\n\n\n“00Mf”\n713\n\n\n“British Library”\n525\n\n\n“Inserted article about the fires from the Courant after the title page.”\n1\n\n\n“National Library of Scotland”\n310\n\n\n“StEdNL”\n1\n\n\n“UkOxU”\n33\n\n\n\n\n\n6.1.4 libraries003.csv\nList the content of the 003 (it is useful only if the catalog is an aggregated catalog). Its columns are:\n\nlibrary: the code of a library\ncount: the number of records having a particular library code\n\n\n\n\nlibrary\ncount\n\n\n\n\n“103861”\n1\n\n\n“BA-SaUP”\n143\n\n\n“BoCbLA”\n25\n\n\n“CStRLIN”\n110\n\n\n“DLC”\n3\n\n\n\n\n\n6.1.5 completeness.params.json\nThe list of the actual parameters in analysis.\nAn example with parameters used for analysing a MARC dataset. When the input is a complex expression it is displayed here in a parsed format. It also contains some metadata such as the versions of MQFA API and QA catalogue.\n{\n  \"args\":[\"/path/to/input.xml.gz\"],\n  \"marcVersion\":\"MARC21\",\n  \"marcFormat\":\"XML\",\n  \"dataSource\":\"FILE\",\n  \"limit\":-1,\n  \"offset\":-1,\n  \"id\":null,\n  \"defaultRecordType\":\"BOOKS\",\n  \"alephseq\":false,\n  \"marcxml\":true,\n  \"lineSeparated\":false,\n  \"trimId\":false,\n  \"outputDir\":\"/path/to/_output/\",\n  \"recordIgnorator\":{\n    \"conditions\":null,\n    \"empty\":true\n  },\n  \"recordFilter\":{\n    \"conditions\":null,\n    \"empty\":true\n  },\n  \"ignorableFields\":{\n    \"fields\":null,\n    \"empty\":true\n  },\n  \"stream\":null,\n  \"defaultEncoding\":null,\n  \"alephseqLineType\":null,\n  \"picaIdField\":\"003@$0\",\n  \"picaSubfieldSeparator\":\"$\",\n  \"picaSchemaFile\":null,\n  \"picaRecordTypeField\":\"002@$0\",\n  \"schemaType\":\"MARC21\",\n  \"groupBy\":null,\n  \"groupListFile\":null,\n  \"format\":\"COMMA_SEPARATED\",\n  \"advanced\":false,\n  \"onlyPackages\":false,\n  \"replacementInControlFields\":\"#\",\n  \"marc21\":true,\n  \"pica\":false,\n  \"mqaf.version\":\"0.9.2\",\n  \"qa-catalogue.version\":\"0.7.0\"\n}",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Completeness</span>"
    ]
  },
  {
    "objectID": "completeness.html#output-files-for-union-catalogues",
    "href": "completeness.html#output-files-for-union-catalogues",
    "title": "6  Completeness",
    "section": "6.2 Output files for union catalogues",
    "text": "6.2 Output files for union catalogues\nFor union catalogues the marc-elements.csv and packages.csv have a special version.\n\n6.2.1 completeness-grouped-marc-elements.csv\nThe same as marc-elements.csv but with an extra element groupId\n\ngroupId: the library identifier available in the data element specified by the --groupBy parameter. 0 has a special meaning: all libraries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroupId\ndocumenttype\npath\npackageid\npackage\ntag\nsubfield\nnumber-of-record\nnumber-of-instances\nmin\nmax\nmean\nstddev\nhistogram\n\n\n\n\n350\nall\n044K$9\n50\nPICA+ bibliographic description\n“Schlagwortfolgen (GBV, SWB, K10plus)”\nPPN\n1\n1\n1\n1\n1.0\n0.0\n1=1\n\n\n350\nall\n044K$7\n50\nPICA+ bibliographic description\n“Schlagwortfolgen (GBV, SWB, K10plus)”\nVorläufiger Link\n1\n1\n1\n1\n1.0\n0.0\n1=1\n\n\n\n\n\n6.2.2 completeness-grouped-packages.csv\nThe same as packages.csv but with an extra element group\n\ngroup: the library identifier available in the data element specified by the --groupBy parameter. 0 has a special meaning: all libraries\n\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\ndocumenttype\npackageid\nname\nlabel\niscoretag\ncount\n\n\n\n\n0\nDruckschriften (einschließlich Bildbänden)\n50\n0…\nPICA+ bibliographic description\nfalse\n987\n\n\n0\nDruckschriften (einschließlich Bildbänden)\n99\nunknown\nunknown origin\nfalse\n3\n\n\n0\nMedienkombination\n50\n0…\nPICA+ bibliographic description\nfalse\n1\n\n\n0\nMikroform\n50\n0…\nPICA+ bibliographic description\nfalse\n11\n\n\n0\nTonträger, Videodatenträger, Bildliche Darstellungen\n50\n0…\nPICA+ bibliographic description\nfalse\n1\n\n\n0\nall\n50\n0…\nPICA+ bibliographic description\nfalse\n1000\n\n\n0\nall\n99\nunknown\nunknown origin\nfalse\n3\n\n\n100\nDruckschriften (einschließlich Bildbänden)\n50\n0…\nPICA+ bibliographic description\nfalse\n20\n\n\n100\nMedienkombination\n50\n0…\nPICA+ bibliographic description\nfalse\n1\n\n\n\n\n\n6.2.3 completeness-groups.csv\nThis is available for union catalogues, containing the groups\n\nid: the group identifier\ngroup: the name of the library\ncount: the number of records from the particular library\n\n\n\n\n\n\n\n\n\nid\ngroup\ncount\n\n\n\n\n0\nall\n1000\n\n\n100\nOtto-von-Guericke-Universität, Universitätsbibliothek Magdeburg [DE-Ma9]\n21\n\n\n1003\nKreisarchäologie Rotenburg [DE-MUS-125322…]\n1\n\n\n101\nOtto-von-Guericke-Universität, Universitätsbibliothek, Medizinische Zentralbibliothek (MZB), Magdeburg [DE-Ma14…]\n6\n\n\n1012\nMariengymnasium Jever [DE-Je1]\n19\n\n\n\n\n\n6.2.4 id-groupid.csv\nThis is the very same file what validation creates. Completeness creates it only if it is not yet available.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Completeness</span>"
    ]
  },
  {
    "objectID": "completeness.html#post-processing-completeness-result-completeness-sqlite",
    "href": "completeness.html#post-processing-completeness-result-completeness-sqlite",
    "title": "6  Completeness",
    "section": "6.3 post processing completeness result (completeness-sqlite)",
    "text": "6.3 post processing completeness result (completeness-sqlite)\nThe completeness-sqlite step (which is launched by the completeness step, but could be launched independently as well) imports marc-elements.csv or completeness-grouped-marc-elements.csv file into marc_elements table. For the catalogues without the --groupBy parameter the groupId column will be filled by 0. Its columns are:\ngroupId             INTEGER,\ndocumenttype        TEXT,\npath                TEXT,\npackageid           INTEGER,\npackage             TEXT,\ntag                 TEXT,\nsubfield            TEXT,\nnumber-of-record    INTEGER,\nnumber-of-instances INTEGER,\nmin                 INTEGER,\nmax                 INTEGER,\nmean                REAL,\nstddev              REAL,\nhistogram           TEXT",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Completeness</span>"
    ]
  },
  {
    "objectID": "thompson-traill-completeness.html",
    "href": "thompson-traill-completeness.html",
    "title": "7  Thompson-Traill completeness",
    "section": "",
    "text": "Kelly Thompson and Stacie Traill recently published their approach to calculate the quality of ebook records coming from different data sources. Their article is Implementation of the scoring algorithm described in Leveraging Python to improve ebook metadata selection, ingest, and management. In Code4Lib Journal, Issue 38, 2017-10-18. http://journal.code4lib.org/articles/12828\njava -cp $JAR de.gwdg.metadataqa.marc.cli.ThompsonTraillCompleteness [options] &lt;file&gt;\nor with a bash script\n./tt-completeness [options] &lt;file&gt;\nor\ncatalogues/[catalogue].sh tt-completeness\nor\n./qa-catalogue --params=\"[options]\" tt-completeness\noptions:\n\ngeneral parameters\n-F &lt;file&gt;, --fileName &lt;file&gt;: the name of report the program produces. Default is tt-completeness.csv.\n\nIt produces a CSV file like this:\nid,ISBN,Authors,Alternative Titles,Edition,Contributors,Series,TOC,Date 008,Date 26X,LC/NLM, \\\nLoC,Mesh,Fast,GND,Other,Online,Language of Resource,Country of Publication,noLanguageOrEnglish, \\\nRDA,total\n\"010002197\",0,0,0,0,0,0,0,1,2,0,0,0,0,0,0,0,1,0,0,0,4\n\"01000288X\",0,0,1,0,0,1,0,1,2,0,0,0,0,0,0,0,0,0,0,0,5\n\"010004483\",0,0,1,0,0,0,0,1,2,0,0,0,0,0,0,0,1,0,0,0,5\n\"010018883\",0,0,0,0,1,0,0,1,2,0,0,0,0,0,0,0,1,1,0,0,6\n\"010023623\",0,0,3,0,0,0,0,1,2,0,0,0,0,0,0,0,1,0,0,0,7\n\"010027734\",0,0,3,0,1,2,0,1,2,0,0,0,0,0,0,0,1,0,0,0,10",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Thompson-Traill completeness</span>"
    ]
  },
  {
    "objectID": "shelf-ready-completeness.html",
    "href": "shelf-ready-completeness.html",
    "title": "8  Shelf-ready completeness",
    "section": "",
    "text": "This analysis is the implementation of the following paper:\nEmma Booth (2020) Quality of Shelf-Ready Metadata. Analysis of survey responses and recommendations for suppliers Pontefract (UK): National Acquisitions Group, 2020. p 31. https://nag.org.uk/wp-content/uploads/2020/06/NAG-Quality-of-Shelf-Ready-Metadata-Survey-Analysis-and-Recommendations_FINAL_June2020.pdf\nThe main purpose of the report is to highlight which fields of the printed and electronic book records are important when the records are coming from different suppliers. 50 libraries participated in the survey, each selected which fields are important to them. The report listed those fields which gets the highest scores.\nThe current calculation based on this list of essential fields. If all data elements specified are available in the record it gets the full score, if only some of them, it gets a proportional score. E.g. under 250 (edition statement) there are two subfields. If both are available, it gets score 44. If only one of them, it gets the half of it, 22, and if none, it gets 0. For 1XX, 6XX, 7XX and 8XX the record gets the full scores if at least one of those fields (with subfield $a) is available. The total score became the average. The theoretical maximum score would be 28.44, which could be accessed if all the data elements are available in the record.\njava -cp $JAR de.gwdg.metadataqa.marc.cli.ShelfReadyCompleteness [options] &lt;file&gt;\nwith a bash script\n./shelf-ready-completeness [options] &lt;file&gt;\nor\ncatalogues/[catalogue].sh shelf-ready-completeness\nor\n./qa-catalogue --params=\"[options]\" shelf-ready-completeness\noptions:\n\ngeneral parameters\n-F &lt;file&gt;, --fileName &lt;file&gt;: the report file name (default is shelf-ready-completeness.csv)",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Shelf-ready completeness</span>"
    ]
  },
  {
    "objectID": "serial-score-analysis.html",
    "href": "serial-score-analysis.html",
    "title": "9  Serial score analysis",
    "section": "",
    "text": "These scores are calculated for each continuing resources (type of record (LDR/6) is language material (‘a’) and bibliographic level (LDR/7) is serial component part (‘b’), integrating resource (‘i’) or serial (‘s’)).\nThe calculation is based on a slightly modified version of the method published by Jamie Carlstone in the following paper:\nJamie Carlstone (2017) Scoring the Quality of E-Serials MARC Records Using Java, Serials Review, 43:3-4, pp. 271-277, DOI: 10.1080/00987913.2017.1350525\njava -cp $JAR de.gwdg.metadataqa.marc.cli.SerialScore [options] &lt;file&gt;\nwith a bash script\n./serial-score [options] &lt;file&gt;\nor\ncatalogues/[catalogue].sh serial-score\nor\n./qa-catalogue --params=\"[options]\" serial-score\noptions:\n\ngeneral parameters\n-F &lt;file&gt;, --fileName &lt;file&gt;: the report file name. Default is shelf-ready-completeness.csv.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Serial score analysis</span>"
    ]
  },
  {
    "objectID": "FRBR-functional-requirements.html",
    "href": "FRBR-functional-requirements.html",
    "title": "10  FRBR functional requirement analysis",
    "section": "",
    "text": "10.1 Discovery functions\nThe Functional Requirements for Bibliographic Records (FRBR) document’s main part defines the primary and secondary entities which became famous as FRBR models. Years later Tom Delsey created a mapping between the 12 functions and the individual MARC elements.\nTom Delsey (2002) Functional analysis of the MARC 21 bibliographic and holdings formats. Tech. report. Library of Congress, 2002. Prepared for the Network Development and MARC Standards Office Library of Congress. Second Revision: September 17, 2003. https://www.loc.gov/marc/marc-functional-analysis/original_source/analysis.pdf.\nThis analysis shows how these functions are supported by the records. Low support means that only small portion of the fields support a function are available in the records, strong support on the contrary means lots of fields are available. The analyses calculate the support of 12 functions for each record, and returns summary statistics.\nIt is an experimental feature because it turned out, that the mapping covers about 2000 elements (fields, subfields, indicators etc.), however on an average record there are max several hundred elements, which results that even in the best record has about 10-15% of the totality of the elements supporting a given function. So the tool doesn’t show you exact numbers, and the scale is not 0-100 but 0-[best score] which is different for every catalogue.\nThe 12 functions:",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>FRBR functional requirement analysis</span>"
    ]
  },
  {
    "objectID": "FRBR-functional-requirements.html#discovery-functions",
    "href": "FRBR-functional-requirements.html#discovery-functions",
    "title": "10  FRBR functional requirement analysis",
    "section": "",
    "text": "search (DiscoverySearch): Search for a resource corresponding to stated criteria (i.e., to search either a single entity or a set of entities using an attribute or relationship of the entity as the search criteria).\nidentify (DiscoveryIdentify): Identify a resource (i.e., to confirm that the entity described or located corresponds to the entity sought, or to distinguish between two or more entities with similar characteristics).\nselect (DiscoverySelect): Select a resource that is appropriate to the user’s needs (i.e., to choose an entity that meets the user’s requirements with respect to content, physical format, etc., or to reject an entity as being inappropriate to the user’s needs)\nobtain (DiscoveryObtain): Access a resource either physically or electronically through an online connection to a remote computer, and/or acquire a resource through purchase, licence, loan, etc.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>FRBR functional requirement analysis</span>"
    ]
  },
  {
    "objectID": "FRBR-functional-requirements.html#usage-functions",
    "href": "FRBR-functional-requirements.html#usage-functions",
    "title": "10  FRBR functional requirement analysis",
    "section": "10.2 Usage functions",
    "text": "10.2 Usage functions\n\nrestrict (UseRestrict): Control access to or use of a resource (i.e., to restrict access to and/or use of an entity on the basis of proprietary rights, administrative policy, etc.).\nmanage (UseManage): Manage a resource in the course of acquisition, circulation, preservation, etc.\noperate (UseOperate): Operate a resource (i.e., to open, display, play, activate, run, etc. an entity that requires specialized equipment, software, etc. for its operation).\ninterpret (UseInterpret): Interpret or assess the information contained in a resource.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>FRBR functional requirement analysis</span>"
    ]
  },
  {
    "objectID": "FRBR-functional-requirements.html#management-functions",
    "href": "FRBR-functional-requirements.html#management-functions",
    "title": "10  FRBR functional requirement analysis",
    "section": "10.3 Management functions",
    "text": "10.3 Management functions\n\nidentify (ManagementIdentify): Identify a record, segment, field, or data element (i.e., to differentiate one logical data component from another).\nprocess (ManagementProcess): Process a record, segment, field, or data element (i.e., to add, delete, replace, output, etc. a logical data component by means of an automated process).\nsort (ManagementSort): Sort a field for purposes of alphabetic or numeric arrangement.\ndisplay (ManagementDisplay): Display a field or data element (i.e., to display a field or data element with the appropriate print constant or as a tracing).\n\njava -cp $JAR de.gwdg.metadataqa.marc.cli.FunctionalAnalysis [options] &lt;file&gt;\nwith a bash script\n./functional-analysis [options] &lt;file&gt;\nor\ncatalogues/&lt;catalogue&gt;.sh functional-analysis\nor\n./qa-catalogue --params=\"[options]\" functional-analysis\noptions: * general parameters\nOutput files: * functional-analysis.csv: the list of the 12 functions and their average count (number of support fields), and average score (percentage of all supporting fields available in the record) * functional-analysis-mapping.csv: the mapping of functions and data elements * functional-analysis-histogram.csv: the histogram of scores and count of records for each function (e.g. there are x number of records which has j score for function a)",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>FRBR functional requirement analysis</span>"
    ]
  },
  {
    "objectID": "classification.html",
    "href": "classification.html",
    "title": "11  Classification analysis",
    "section": "",
    "text": "It analyses the coverage of subject indexing/classification in the catalogue. It checks specific fields, which might have subject indexing information, and provides details about how and which subject indexing schemes have been applied.\njava -cp $JAR de.gwdg.metadataqa.marc.cli.ClassificationAnalysis [options] &lt;file&gt;\nRscript scripts/classifications/classifications-type.R &lt;output directory&gt;\nwith a bash script\n./classifications [options] &lt;file&gt;\nRscript scripts/classifications/classifications-type.R &lt;output directory&gt;\nor\ncatalogues/[catalogue].sh classifications\nor\n./qa-catalogue --params=\"[options]\" classifications\noptions:\n\ngeneral parameters\n-w, --emptyLargeCollectors: empty large collectors periodically. It is a memory optimization parameter, turn it on if you run into a memory problem.\n\nThe output is a set of files:\n\nclassifications-by-records.csv: general overview of how many records has any subject indexing\nclassifications-by-schema.csv: which subject indexing schemas are available in the catalogues (such as DDC, UDC, MESH etc.) and where they are referred\nclassifications-histogram.csv: a frequency distribution of the number of subjects available in records (x records have 0 subjects, y records have 1 subjects, z records have 2 subjects etc.)\nclassifications-frequency-examples.csv: examples for particular distributions (one record ID which has 0 subject, one which has 1 subject, etc.)\nclassifications-by-schema-subfields.csv: the distribution of subfields of those fields, which contains subject indexing information. It gives you a background that what other contextual information behind the subject term are available (such as the version of the subject indexing scheme)\nclassifications-collocations.csv: how many record has a particular set of subject indexing schemes\nclassifications-by-type.csv: returns the subject indexing schemes and their types in order of the number of records. The types are TERM_LIST (subtypes: DICTIONARY, GLOSSARY, SYNONYM_RING), METADATA_LIKE_MODEL (NAME_AUTHORITY_LIST, GAZETTEER), CLASSIFICATION (SUBJECT_HEADING, CATEGORIZATION, TAXONOMY, CLASSIFICATION_SCHEME), RELATIONSHIP_MODEL (THESAURUS, SEMANTIC_NETWORK, ONTOLOGY).",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Classification analysis</span>"
    ]
  },
  {
    "objectID": "authority-name-analysis.html",
    "href": "authority-name-analysis.html",
    "title": "12  Authority name analysis",
    "section": "",
    "text": "It analyses the coverage of authority names (persons, organisations, events, uniform titles) in the catalogue. It checks specific fields, which might have authority names, and provides details about how and which schemes have been applied.\njava -cp $JAR de.gwdg.metadataqa.marc.cli.AuthorityAnalysis [options] &lt;file&gt;\nwith a bash script\n./authorities [options] &lt;file&gt;\nor\ncatalogues/&lt;catalogue&gt;.sh authorities\nor\n./qa-catalogue --params=\"[options]\" authorities\noptions:\n\ngeneral parameters\n-w, --emptyLargeCollectors: empty large collectors periodically. It is a memory optimization parameter, turn it on if you run into a memory problem\n\nThe output is a set of files:\n\nauthorities-by-records.csv: general overview of how many records has any authority names\nauthorities-by-schema.csv: which authority names schemas are available in the catalogues (such as ISNI, Gemeinsame Normdatei etc.) and where they are referred\nauthorities-histogram.csv: a frequency distribution of the number of authority names available in records (x records have 0 authority names, y records have 1 authority name, z records have 2 authority names etc.)\nauthorities-frequency-examples.csv: examples for particular distributions (one record ID which has 0 authority name, one which has 1 authority name, etc.)\nauthorities-by-schema-subfields.csv: the distribution of subfields of those fields, which contains authority names information. It gives you a background that what other contextual information behind the authority names are available (such as the version of the authority name scheme)",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Authority name analysis</span>"
    ]
  },
  {
    "objectID": "field-frequency-distribution.html",
    "href": "field-frequency-distribution.html",
    "title": "13  Field frequency distribution",
    "section": "",
    "text": "This analysis reveals the relative importance of some fields. Pareto’s distribution is a kind of power law distribution, and Pareto-rule of 80-20 rules states that 80% of outcomes are due to 20% of causes. In catalogue outcome is the total occurrences of the data element, causes are individual data elements. In catalogues some data elements occurs much more frequently then others. This analyses highlights the distribution of the data elements: whether it is similar to Pareto’s distribution or not.\nIt produces charts for each document type and one for the whole catalogue showing the field frequency patterns. Each chart shows a line which is the function of field frequency: on the X-axis you can see the subfields ordered by the frequency (how many times a given subfield occurred in the whole catalogue). They are ordered by frequency from the most frequent top 1% to the least frequent 1% subfields. The Y-axis represents the cumulative occurrence (from 0% to 100%).\nBefore running it you should first run the completeness calculation.\nWith a bash script\ncatalogues/[catalogue].sh pareto\nor\n./qa-catalogue --params=\"[options]\" pareto\noptions:\n\ngeneral parameters",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Field frequency distribution</span>"
    ]
  },
  {
    "objectID": "generating-cataloguing-history-chart.html",
    "href": "generating-cataloguing-history-chart.html",
    "title": "14  Generating cataloguing history chart",
    "section": "",
    "text": "This analysis is based on Benjamin Schmidt’s blog post A brief visual history of MARC cataloging at the Library of Congress. (Tuesday, May 16, 2017).\nIt produces a chart where the Y-axis is based on the “date entered on file” data element that indicates the date the MARC record was created (008/00-05), the X-axis is based on “Date 1” element (008/07-10).\nUsage:\ncatalogues/[catalogue].sh marc-history\nor\n./qa-catalogue --params=\"[options]\" marc-history\noptions:\n\ngeneral parameters",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Generating cataloguing history chart</span>"
    ]
  },
  {
    "objectID": "import-tables-to-SQLite.html",
    "href": "import-tables-to-SQLite.html",
    "title": "15  Import tables to SQLite",
    "section": "",
    "text": "This is just a helper function which imports the results of validation into SQLite3 database.\nThe prerequisite of this step is to run validation first, since it uses the files produced there. If you run validation with catalogues/&lt;catalogue&gt;.sh or ./qa-catalogue scripts, this importing step is already covered there.\nUsage:\ncatalogues/[catalogue].sh sqlite\nor\n./qa-catalogue --params=\"[options]\" sqlite\noptions:\n\ngeneral parameters\n\nOutput:\n\nqa_catalogue.sqlite: the SQLite3 database with 3 tables: issue_details, issue_groups, and issue_summary.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Import tables to SQLite</span>"
    ]
  },
  {
    "objectID": "indexing.html",
    "href": "indexing.html",
    "title": "16  Indexing bibliographic records with Solr",
    "section": "",
    "text": "16.1 Solr field names\nRun indexer:\nWith script:\nor\noptions:\nThe ./index file (which is used by catalogues/[catalogue].sh and ./qa-catalogue scripts) has additional parameters: * -Z &lt;core&gt;, --core &lt;core&gt;: The index name (core). If not set it will be extracted from the solrUrl parameter * -Y &lt;path&gt;, --file-path &lt;path&gt;: File path * -X &lt;mask&gt;, --file-mask &lt;mask&gt;: File mask * -W, --purge: Purge index and exit * -V, --status: Show the status of index(es) and exit * -U, --no-delete: Do not delete documents in index before starting indexing (be default the script clears the index)\nQA catalogue builds a Solr index which contains a) a set of fixed Solr fields that are the same for all bibliographic input, and b) Solr fields that depend on the field names of the metadata schema (MARC, PICA, UNIMARC etc.) - these fields should be mapped from metadata schema to dynamic Solr fields by an algorithm.",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Indexing bibliographic records with Solr</span>"
    ]
  },
  {
    "objectID": "indexing.html#solr-field-names",
    "href": "indexing.html#solr-field-names",
    "title": "16  Indexing bibliographic records with Solr",
    "section": "",
    "text": "16.1.1 Fixed fields\n\nid: the record ID. This comes from the identifier of the bibliographic record, so 001 for MARC21\nrecord_sni: the JSON representation of the bibliographic record\ngroupId_is: the list of group IDs. The content comes from the data element specified by the --groupBy parameter split by commas (‘,’).\nerrorId_is: the list of error IDs that come from the result of the validation.\n\n\n\n16.1.2 Mapped fields\nThe mapped fields are Solr fields that depend on the field names of the metadata schema. The final Solr field follows the pattern:\n\nField prefix:\nWith --fieldPrefix parameter you can set a prefix that is applied to the variable fields. This might be needed because Solr has a limitation: field names start with a number can not be used in some Solr parameter, such as fl (field list selected to be retrieved from the index). Unfortunately bibliographic schemas use field names start with numbers. You can change a mapping parameter that produces a mapped value that resembles the BIBFRAME mapping of the MARC21 field, but not all field has such a human readable association.\nField suffixes:\n\n*_sni: not indexed, stored string fields – good for storing fields used for displaying information\n*_ss: not parsed, stored, indexed string fields – good for display and facets\n*_tt: parsed, not stored, indexed string fields – good for term searches (these fields will be availabe if --indexWithTokenizedField parameter is applied)\n*_is: parsed, not stored, indexed integer fields – good for searching for numbers, such as error or group identifiers (these fields will be availabe if --indexFieldCounts parameter is applied)\n\nThe mapped value\nWith --solrFieldType you can select the algorithm that generates the mapped value. Right now there are three formats: * marc-tags - the field names are MARC codes (245$a → 245a) * human-readable - the field names are Self Descriptive MARC code (245$a → Title_mainTitle) * mixed - the field names are mixed of the above (e.g. 245a_Title_mainTitle)\n\n16.1.2.1 “marc-tags” format\n\"100a_ss\":[\"Jung-Baek, Myong Ja\"],\n\"100ind1_ss\":[\"Surname\"],\n\"245c_ss\":[\"Vorgelegt von Myong Ja Jung-Baek.\"],\n\"245ind2_ss\":[\"No nonfiling characters\"],\n\"245a_ss\":[\"S. Tret'jakov und China /\"],\n\"245ind1_ss\":[\"Added entry\"],\n\"260c_ss\":[\"1987.\"],\n\"260b_ss\":[\"Georg-August-Universität Göttingen,\"],\n\"260a_ss\":[\"Göttingen :\"],\n\"260ind1_ss\":[\"Not applicable/No information provided/Earliest available publisher\"],\n\"300a_ss\":[\"141 p.\"],\n\n\n16.1.2.2 “human-readable” format\n\"MainPersonalName_type_ss\":[\"Surname\"],\n\"MainPersonalName_personalName_ss\":[\"Jung-Baek, Myong Ja\"],\n\"Title_responsibilityStatement_ss\":[\"Vorgelegt von Myong Ja Jung-Baek.\"],\n\"Title_mainTitle_ss\":[\"S. Tret'jakov und China /\"],\n\"Title_titleAddedEntry_ss\":[\"Added entry\"],\n\"Title_nonfilingCharacters_ss\":[\"No nonfiling characters\"],\n\"Publication_sequenceOfPublishingStatements_ss\":[\"Not applicable/No information provided/Earliest available publisher\"],\n\"Publication_agent_ss\":[\"Georg-August-Universität Göttingen,\"],\n\"Publication_place_ss\":[\"Göttingen :\"],\n\"Publication_date_ss\":[\"1987.\"],\n\"PhysicalDescription_extent_ss\":[\"141 p.\"],\n\n\n16.1.2.3 “mixed” format\n\"100a_MainPersonalName_personalName_ss\":[\"Jung-Baek, Myong Ja\"],\n\"100ind1_MainPersonalName_type_ss\":[\"Surname\"],\n\"245a_Title_mainTitle_ss\":[\"S. Tret'jakov und China /\"],\n\"245ind1_Title_titleAddedEntry_ss\":[\"Added entry\"],\n\"245ind2_Title_nonfilingCharacters_ss\":[\"No nonfiling characters\"],\n\"245c_Title_responsibilityStatement_ss\":[\"Vorgelegt von Myong Ja Jung-Baek.\"],\n\"260b_Publication_agent_ss\":[\"Georg-August-Universität Göttingen,\"],\n\"260a_Publication_place_ss\":[\"Göttingen :\"],\n\"260ind1_Publication_sequenceOfPublishingStatements_ss\":[\"Not applicable/No information provided/Earliest available publisher\"],\n\"260c_Publication_date_ss\":[\"1987.\"],\n\"300a_PhysicalDescription_extent_ss\":[\"141 p.\"],\nA distinct project metadata-qa-marc-web, provides a web application that utilizes to build this type of Solr index in number of ways (a facetted search interface, term lists, search for validation errors etc.)",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Indexing bibliographic records with Solr</span>"
    ]
  },
  {
    "objectID": "indexing.html#index-preparation",
    "href": "indexing.html#index-preparation",
    "title": "16  Indexing bibliographic records with Solr",
    "section": "16.2 Index preparation",
    "text": "16.2 Index preparation\nThe tool uses different Solr indices (aka cores) to store information. In the following example we use loc as the name of our catalogue. There are two main indices: loc and loc_dev. loc_dev is the target of the index process, it will create it from scratch. During the proess loc is available and searchable. When the indexing has been successfully finished these two indices will be swaped, so the previous loc will become loc_dev, and the new index will be loc. The web user interface will always use the latest version (not the dev).\nBesides these two indices there is a third index that contains different kind of results of the analyses. At the time of writing it contains only the results of validation, but later it will cover other information as well. It can be set by the following parameter:\n\n-4, --solrForScoresUrl &lt;arg&gt;: the URL of the Solr server used to store scores (it is populated in the validate-sqlite process which runs after validation)\n\nDuring the indexing process the content of this index is meged into the _dev index, so after a successfull end of the process this index is not needed anymore.\nIn order to make the automation easier and still flexible there are some an auxilary commands:\n\n./qa-catalogue prepare-solr: created these two indices, makes sure that their schemas contain the necessary fields\n./qa-catalogue index: runs the indexing process\n./qa-catalogue postprocess-solr: swap the two Solr cores ( and _dev)\n./qa-catalogue all-solr: runs all the three steps\n\nIf you would like to maintain the Solr index yourself (e.g. because the Solr instance wuns in a cloud environment), you should skip prepare-solr and postprocess-solr, and run only index. For maintaining the schema you can find a minimal viable schema among the test resources\nYou can set autocommit the following way in solrconfig.xml (inside Solr):\n&lt;autoCommit&gt;\n  &lt;maxTime&gt;${solr.autoCommit.maxTime:15000}&lt;/maxTime&gt;\n  &lt;maxDocs&gt;5000&lt;/maxDocs&gt;\n  &lt;openSearcher&gt;true&lt;/openSearcher&gt;\n&lt;/autoCommit&gt;\n...\n&lt;autoSoftCommit&gt;\n  &lt;maxTime&gt;${solr.autoSoftCommit.maxTime:-1}&lt;/maxTime&gt;\n&lt;/autoSoftCommit&gt;\nIt needs if you choose to disable QA catalogue to issue commit messages (see --commitAt parameter), which makes indexing faster.\nIn schema.xml (or in Solr web interface) you should be sure that you have the following dynamic fields:\n&lt;dynamicField name=\"*_ss\" type=\"strings\" indexed=\"true\" stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_tt\" type=\"text_general\" indexed=\"true\" stored=\"false\"/&gt;\n&lt;dynamicField name=\"*_is\" type=\"pints\" indexed=\"true\" stored=\"true\" /&gt;\n&lt;dynamicField name=\"*_sni\" type=\"string_big\" docValues=\"false\" multiValued=\"false\" indexed=\"false\" stored=\"true\"/&gt;\n\n&lt;copyField source=\"*_ss\" dest=\"_text_\"/&gt;\nor use Solr API:\nNAME=dnb\nSOLR=http://localhost:8983/solr/$NAME/schema\n\n// add copy field\ncurl -X POST -H 'Content-type:application/json' --data-binary '{\n  \"add-dynamic-field\":{\n     \"name\":\"*_sni\",\n     \"type\":\"string\",\n     \"indexed\":false,\n     \"stored\":true}\n}' $SOLR\n\ncurl -X POST -H 'Content-type:application/json' --data-binary '{\n  \"add-copy-field\":{\n     \"source\":\"*_ss\",\n     \"dest\":[\"_text_\"]}\n}' $SOLR\n...\nSee the solr-functions file for full code.\nQA catalogue has a helper scipt to get information about the status of Solr index (Solr URL, location, the list of cores, number of documents, size in the disk, and last modification):\n$ ./index --status\nSolr index status at http://localhost:8983\nSolr directory: /opt/solr-9.3.0/server/solr\n\ncore                 | location        | nr of docs |       size |       last modified\n.................... | ............... | .......... | .......... | ...................\nnls                  | nls_1           |     403946 | 1002.22 MB | 2023-11-25 21:59:39\nnls_dev              | nls_2           |     403943 |  987.22 MB | 2023-11-11 15:59:49\nnls_validation       | nls_validation  |     403946 |   17.89 MB | 2023-11-25 21:35:44\nyale                 | yale_2          |    2346976 |    9.51 GB | 2023-11-11 13:12:35\nyale_dev             | yale_1          |    2346976 |    9.27 GB | 2023-11-11 10:58:08",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Indexing bibliographic records with Solr</span>"
    ]
  },
  {
    "objectID": "avram-schemas.html",
    "href": "avram-schemas.html",
    "title": "17  Avram Schemas",
    "section": "",
    "text": "17.1 Export Java classes to MARC21 schemas\nThe definition of valid MARC21 in QA catalogue is partly hard-coded in form of Java classes. The Avram Schema format has been developed to encode information about MARC and related formats independent from implementations (see MARC21 structure in JSON for some historical background). The definition of PICA-based formats and UNIMARC has already been moved to Avram Schemas. Repository directory avram-schemas contains the following schema files:\nTo check schema files against the Avram specification. Install a node script with npm ci and run:\nTo generate HTML display of schema files (see Self Descriptive MARC code for an earlier version):\nThe script export-schema, implemented in de.gwdg.metadataqa.marc.cli.utils.MappingToJson can be used to export an Avram Schema for MARC21 from Java classes. Some additional, non-standard information is included on request.\nOptions:\nTo export the most used forms of the schema into into directory avram-schemas/, call:\nThe script version generates 3 files, with different details:",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Avram Schemas</span>"
    ]
  },
  {
    "objectID": "avram-schemas.html#export-java-classes-to-marc21-schemas",
    "href": "avram-schemas.html#export-java-classes-to-marc21-schemas",
    "title": "17  Avram Schemas",
    "section": "",
    "text": "-c, --withSubfieldCodelists: with subfield codelists\n-s, --withSelfDescriptiveCode: with self-descriptive codes\n-t &lt;type&gt;, --solrFieldType &lt;type&gt;: type of Solr fields, could be one of marc-tags, human-readable, or mixed\n-f, --withFrbrFunctions: with FRBR functions (see Tom Delsey: Functional analysis of the MARC 21 bibliographic and holdings formats. Tech. report, 2nd revision. Library of Congress, 2003.)\n-l, --withComplianceLevel: with compliance levels (national, minimal) (see National Level Full and Minimal Requirements. Library of Congress, 1999.)\n\n\n./qa-catalogue export-schema-files\n\n\navram-schemas/marc.json (valid Avram)\navram-schemas/marc-schema-with-solr.json\navram-schemas/marc-schema-with-solr-and-extensions.json\n\n\n\n\nListing 17.1: An excerpt of the MARC21 Avram Schema with extension compliance-level, frbr-functions, and code lists\n\n\n{\n\"010\":{\n  \"tag\":\"010\",\n  \"label\":\"Library of Congress Control Number\",\n  \"url\":\"https://www.loc.gov/marc/bibliographic/bd010.html\",\n  \"repeatable\":false,\n  \"compilance-level\":{\n    \"national\":\"Mandatory if applicable\",\n    \"minimal\":\"Mandatory if applicable\"\n  },\n  \"indicator1\":null,\n  \"indicator2\":null,\n  \"subfields\":{\n    \"a\":{\n      \"label\":\"LC control number\",\n      \"repeatable\":false,\n      \"frbr-functions\":[\n        \"Data Management/Identify\",\n        \"Data Management/Process\"\n      ],\n      \"compilance-level\":{\n        \"national\":\"Mandatory if applicable\",\n        \"minimal\":\"Mandatory if applicable\"\n      }\n    },\n    ...\n  }\n},\n\"013\":{\n  \"tag\":\"013\",\n  \"label\":\"Patent Control Information\",\n  \"url\":\"https://www.loc.gov/marc/bibliographic/bd013.html\",\n  \"repeatable\":true,\n  \"compilance-level\":{\"national\":\"Optional\"},\n  \"indicator1\":null,\n  \"indicator2\":null,\n  \"subfields\":{\n    \"b\":{\n      \"label\":\"Country\",\n      \"repeatable\":false,\n      \"codelist\":{\n        \"name\":\"MARC Code List for Countries\",\n        \"url\":\"http://www.loc.gov/marc/countries/countries_code.html\",\n        \"codes\":{\n          \"aa\":{\"label\":\"Albania\"},\n          \"abc\":{\"label\":\"Alberta\"},\n          \"-ac\":{\"label\":\"Ashmore and Cartier Islands\"},\n          \"aca\":{\"label\":\"Australian Capital Territory\"},",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Avram Schemas</span>"
    ]
  },
  {
    "objectID": "Shacl4Bib.html",
    "href": "Shacl4Bib.html",
    "title": "18  Shacl4Bib",
    "section": "",
    "text": "since v0.7.0. Note: This is an experimental feature.\nThe Shapes Constraint Language (SHACL) is a formal language for validating Resource Description Framework (RDF) graphs against a set of conditions (expressed also in RDF). Following this idea and implementing a subset of the language, the Metadata Quality Assessment Framework provides a mechanism to define SHACL-like rules for data sources in non-RDF based formats, such as XML, CSV and JSON (SHACL validates only RDF graphs). Shacl4Bib is the extension enabling the validation of bibliographic records. The rules can be defined either with YAML or JSON configuration files or with Java code. SCHACL uses RDF notation to specify or “address” the data element about which the constraints are set. Shacl4Bib supports Carsten Klee’s MARCspec for MARC records, and PICApath for PICA. You can find more information and full definition of the implemented subset of SHACL here: Defining schema with a configuration file\nParameters:\n\n-C &lt;file&gt;, --shaclConfigurationFile &lt;file&gt;: specify the SHACL like configuration file\n-O &lt;file&gt;, --shaclOutputFile &lt;file&gt;: output file (default: shacl4bib.csv)\n-P &lt;type&gt;, --shaclOutputType &lt;type&gt;: specify what the output files should contain. Possible values:\n\nSTATUS: status only (default), where the following values appear:\n\n1 the criteria met,\n0 the criteria have not met,\nNA: the data element is not available in the record),\n\nSCORE: score only. Its value is calculated the following way:\n\nif the criteria met it returns the value of successScore property (or 0 if no such property)\nif the criteria have not met, it returns the value of failureScore property (or 0 if no such property)\n\nBOTH: both status and score\n\n\nHere is a simple example for setting up rules against a MARC subfield:\nformat: MARC\nfields:\n- name: 040$a\n  path: 040$a\n  rules:\n  - id: 040$a.minCount\n    minCount: 1\n  - id: 040$a.pattern\n    pattern: ^BE-KBR00\n\nformat represents the format of the input data. It can be either MARC or PICA\nfields: the list of fields we would like to investigate. Since it is a YAMPL example, the - and indentation denotes child elements. Here there is only one child, so we analyse here a single subfield.\nname is how the data element is called within the rule set. It could be a machine or a human readable string.\npath is the “address” of the metadata element. It should be expressed in an addressing language such as MARCSpec or PICAPath (040$a contains the original cataloging agency)\nrules: the parent element of the set of rules. Here we have two rules.\nid: the identifier of the rule. This will be the header of the column in CSV, and it could be references elsewhere in the SHACL configuration file.\nmintCount: this specify the minimum number of instances of the data element in the record\npattern: a regular expression which should match the values of all instances of the data element\n\nThe output contains an extra column, the record identifier, so it looks like something like this:\nid,040$a.minCount,040$a.pattern\n17529680,1,1\n18212975,1,1\n18216050,1,1\n18184955,1,1\n18184431,1,1\n9550740,NA,NA\n19551181,NA,NA\n118592844,1,1\n18592704,1,1\n18592557,1,1",
    "crumbs": [
      "Usage",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Shacl4Bib</span>"
    ]
  },
  {
    "objectID": "where-can-I-get-MARC-records.html",
    "href": "where-can-I-get-MARC-records.html",
    "title": "Appendix A — Where can I get MARC records?",
    "section": "",
    "text": "A.1 United States of America\nHere is a list of data sources I am aware of so far:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Where can I get MARC records?</span>"
    ]
  },
  {
    "objectID": "where-can-I-get-MARC-records.html#united-states-of-america",
    "href": "where-can-I-get-MARC-records.html#united-states-of-america",
    "title": "Appendix A — Where can I get MARC records?",
    "section": "",
    "text": "Library of Congress — https://www.loc.gov/cds/products/marcDist.php. MARC21 (UTF-8 and MARC8 encoding), MARCXML formats, open access. Alternative access point: https://www.loc.gov/collections/selected-datasets/?fa=contributor:library+of+congress.+cataloging+distribution+service.\nHarvard University Library — https://library.harvard.edu/open-metadata. MARC21 format, CC0. Institution specific features are documented here\nColumbia University Library — https://library.columbia.edu/bts/clio-data.html. 10M records, MARC21 and MARCXML format, CC0.\nUniversity of Michigan Library — https://www.lib.umich.edu/open-access-bibliographic-records. 1,3M records, MARC21 and MARCXML formats, CC0.\nUniversity of Pennsylvania Libraries — https://www.library.upenn.edu/collections/digital-projects/open-data-penn-libraries. Two datasets are available:\n\nCatalog records created by Penn Libraries 572K records, MARCXML format, CC0,\nCatalog records derived from other sources, 6.5M records, MARCXML format, Open Data Commons ODC-BY, use in accordance with the OCLC community norms.\n\nYale University — https://guides.library.yale.edu/c.php?g=923429. Three datasets are available:\n\nYale-originated records: 1.47M records, MARC21 format, CC0\nWorldCat-derived records: 6.16M records, MARC21 format, ODC-BY\nOther records (MARC21), independent of Yale and WorldCat, where sharing is permitted. 404K records, MARC21 format.\n\nNational Library of Medicine (NLM) catalogue records — https://www.nlm.nih.gov/databases/download/catalog.html. 4.2 million records, NLMXML, MARCXML and MARC21 formats. NLM Terms and Conditions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Where can I get MARC records?</span>"
    ]
  },
  {
    "objectID": "where-can-I-get-MARC-records.html#germany",
    "href": "where-can-I-get-MARC-records.html#germany",
    "title": "Appendix A — Where can I get MARC records?",
    "section": "A.2 Germany",
    "text": "A.2 Germany\n\nDeutsche Nationalbibliothek — https://www.dnb.de/DE/Professionell/Metadatendienste/Datenbezug/Gesamtabzuege/gesamtabzuege_node.html (note: the records are provided in utf-8 decomposed). 23.9M records, MARC21 and MARCXML format, CC0.\nBibliotheksverbundes Bayern — https://www.bib-bvb.de/web/b3kat/open-data. 27M records, MARCXML format, CC0.\nLeibniz-Informationszentrum Technik und Naturwissenschaften Universitätsbibliothek (TIB) — https://www.tib.eu/de/forschung-entwicklung/entwicklung/open-data. (no download link, use OAI-PMH instead) Dublin Core, MARC21, MARCXML, CC0.\nK10plus-Verbunddatenbank (K10plus union catalogue of Bibliotheksservice-Zentrum Baden Würtemberg (BSZ) and Gemensamer Bibliotheksverbund (GBV)) — https://swblod.bsz-bw.de/od/. 87M records, MARCXML format, CC0.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Where can I get MARC records?</span>"
    ]
  },
  {
    "objectID": "where-can-I-get-MARC-records.html#others",
    "href": "where-can-I-get-MARC-records.html#others",
    "title": "Appendix A — Where can I get MARC records?",
    "section": "A.3 Others",
    "text": "A.3 Others\n\nUniversiteitsbibliotheek Gent — https://lib.ugent.be/info/exports. Weekly data dump in Aleph Sequential format. It contains some Aleph fields above the standard MARC21 fields. ODC ODbL.\nToronto Public Library — https://opendata.tplcs.ca/. 2.5 million MARC21 records, Open Data Policy\nRépertoire International des Sources Musicales — https://opac.rism.info/index.php?id=8&id=8&L=1. 800K records, MARCXML, RDF/XML, CC-BY.\nETH-Bibliothek (Swiss Federal Institute of Technology in Zurich) — http://www.library.ethz.ch/ms/Open-Data-an-der-ETH-Bibliothek/Downloads. 2.5M records, MARCXML format.\nBritish library — http://www.bl.uk/bibliographic/datafree.html#m21z3950 (no download link, use z39.50 instead after asking for permission). MARC21, usage will be strictly for non-commercial purposes.\nTalis — https://archive.org/details/talis_openlibrary_contribution. 5.5 million MARC21 records contributed by Talis to Open Library under the ODC PDDL.\nOxford Medicine Online (the catalogue of medicine books published by Oxford University Press) — https://oxfordmedicine.com/page/67/. 1790 MARC21 records.\nFennica — the Finnish National Bibliography provided by the Finnish National Library — http://data.nationallibrary.fi/download/. 1 million records, MARCXML, CC0.\nBiblioteka Narodawa (Polish National Library) — https://data.bn.org.pl/databases. 6.5 million MARC21 records.\nMagyar Nemzeti Múzeum (Hungarian National Library) — https://mnm.hu/hu/kozponti-konyvtar/nyilt-bibliografiai-adatok, 67K records, MARC21, HUNMARC, BIBFRAME, CC0\nUniversity of Amsterdam Library — https://uba.uva.nl/en/support/open-data/data-sets-and-publication-channels/data-sets-and-publication-channels.html, 2.7 million records, MARCXML, PDDL/ODC-BY. Note: the record for books are not downloadable, only other document types. One should request them via the website.\nPortugal National Library — https://opendata.bnportugal.gov.pt/downloads.htm. 1.13 million UNIMARC records in MARCXML, RDF XML, JSON, TURTLE and CSV formats. CC0\nNational Library of Latvia National bibliography (2017–2020) — https://dati.lnb.lv/. 11K MARCXML records.\n\nThanks, Johann Rolschewski, Phú, and Hugh Paterson III for their help in collecting this list! Do you know some more data sources? Please let me know.\nThere are two more datasource worth mention, however they do not provide MARC records, but derivatives:\n\nLinked Open British National Bibliography 3.2M book records in N-Triplets and RDF/XML format, CC0 license\nLinked data of Bibliothèque nationale de France. N3, NT and RDF/XML formats, Licence Ouverte/Open Licence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Where can I get MARC records?</span>"
    ]
  },
  {
    "objectID": "handling-MARC-versions.html",
    "href": "handling-MARC-versions.html",
    "title": "Appendix B — Handling MARC versions",
    "section": "",
    "text": "The tool provides two levels of customization:\n\nproject specific tags can be defined in their own Java package, such as these classes for Gent data: de.gwdg.metadataqa.marc.definition.tags.genttags\nfor existing tags one can use the API described below\n\nThe different MARC versions has an identifier. This is defined in the code as an enumeration:\npublic enum MarcVersion {\n  MARC21(\"MARC21\", \"MARC21\"),\n  DNB(\"DNB\", \"Deutsche Nationalbibliothek\"),\n  OCLC(\"OCLC\", \"OCLC\"),\n  GENT(\"GENT\", \"Universiteitsbibliotheek Gent\"),\n  SZTE(\"SZTE\", \"Szegedi Tudományegyetem\"),\n  FENNICA(\"FENNICA\", \"National Library of Finland\")\n  ;\n  ...\n}\nWhen you add version specific modification, you have to use one of these values.\n\nDefining version specific indicator codes:\n\nIndicator::putVersionSpecificCodes(MarcVersion, List&lt;Code&gt;);\nCode is a simple object, it has two property: code and label.\nexample:\npublic class Tag024 extends DataFieldDefinition {\n   ...\n   ind1 = new Indicator(\"Type of standard number or code\")\n             .setCodes(...)\n             .putVersionSpecificCodes(\n                 MarcVersion.SZTE,\n                 Arrays.asList(\n                    new Code(\" \", \"Not specified\")\n                 )\n             )\n   ...\n}\n\nDefining version specific subfields:\n\nDataFieldDefinition::putVersionSpecificSubfields(MarcVersion, List&lt;SubfieldDefinition&gt;)\nSubfieldDefinition contains a definition of a subfield. You can construct it with three String parameters: a code, a label and a cardinality code which denotes whether the subfield can be repeatable (“R”) or not (“NR”).\nexample:\npublic class Tag024 extends DataFieldDefinition {\n   ...\n   putVersionSpecificSubfields(\n      MarcVersion.DNB,\n      Arrays.asList(\n         new SubfieldDefinition(\"9\", \"Standardnummer (mit Bindestrichen)\", \"NR\")\n      )\n   );\n}\n\nMarking indicator codes as obsolete:\n\nIndicator::setHistoricalCodes(List&lt;String&gt;)\nThe list should be pairs of code and description.\npublic class Tag082 extends DataFieldDefinition {\n   ...\n   ind1 = new Indicator(\"Type of edition\")\n              .setCodes(...)\n              .setHistoricalCodes(\n                 \" \", \"No edition information recorded (BK, MU, VM, SE) [OBSOLETE]\",\n                 \"2\", \"Abridged NST version (BK, MU, VM, SE) [OBSOLETE]\"\n              )\n   ...\n}\n\nMarking subfields as obsolete:\n\nDataFieldDefinition::setHistoricalSubfields(List&lt;String&gt;);\nThe list should be pairs of code and description.\n[Java} public class Tag020 extends DataFieldDefinition {    ...    setHistoricalSubfields(       \"b\", \"Binding information (BK, MP, MU) [OBSOLETE]\"    ); }\nIf you create new a package for the new MArc version, you should register it to several places:\n\nadd a case into src/main/java/de/gwdg/metadataqa/marc/Utils.java:\n\ncase \"zbtags\":      version = MarcVersion.ZB;      break;\n\nadd an item into enumeration at src/main/java/de/gwdg/metadataqa/marc/definition/tags/TagCategory.java:\n\nZB(23, \"zbtags\", \"ZB\", \"Locally defined tags of the Zentralbibliothek Zürich\", false),\n\nmodify the expected number of data elements at src/test/java/de/gwdg/metadataqa/marc/utils/DataElementsStaticticsTest.java:\n\nassertEquals( 215, statistics.get(DataElementType.localFields));\n\n… and a src/test/java/de/gwdg/metadataqa/marc/utils/MarcTagListerTest.java:\n\nassertEquals( 2, (int) versionCounter2.get(MarcVersion.ZB));\nassertEquals( 2, (int) versionCounter.get(\"zbtags\"));",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Handling MARC versions</span>"
    ]
  },
  {
    "objectID": "institutions-which-reportedly-use-this-tool.html",
    "href": "institutions-which-reportedly-use-this-tool.html",
    "title": "Appendix C — Institutions which reportedly use this tool",
    "section": "",
    "text": "Universiteitsbibliotheek Gent, Gent, Belgium\nBiblioteksentralen, Oslo, Norway\nDeutsche Digitale Bibliothek, Frankfurt am Main/Berlin, Germany\nBritish Library, London/Boston Spa, United Kingdom\nOrszággyűlési Könyvtár, Budapest, Hungary\nStudijní a vědecká knihovna Plzeňského kraje, Plzeň, Czech Republic\nRoyal Library of Belgium (KBR), Brussels, Belgium\nGemeinsamer Bibliotheksverbund (GBV), Göttingen, Germany\nBinghampton University Libraries, Binghampton, NY, USA\nZentralbibliothek Zürich, Zürich, Switzerland\n\nIf you use this tool as well, please contact me: pkiraly (at) gwdg (dot) de. I really like to hear about your use case and ideas.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Institutions which reportedly use this tool</span>"
    ]
  },
  {
    "objectID": "supporters-and-sponsors.html",
    "href": "supporters-and-sponsors.html",
    "title": "Appendix D — supporters-and-sponsors",
    "section": "",
    "text": "Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG): Hardware, time for research\nGemeinsamer Bibliotheksverbund (GBV): contracting for feature development\nRoyal Library of Belgium (KBR): contracting for feature development\nJetBrains s.r.o.: IntelliJ IDEA development tool community licence",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>supporters-and-sponsors</span>"
    ]
  },
  {
    "objectID": "special-build-process.html",
    "href": "special-build-process.html",
    "title": "Appendix E — Special build process",
    "section": "",
    "text": "“deployment” build (when deploying artifacts to Maven Central)\nmvn clean deploy -Pdeploy",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Special build process</span>"
    ]
  },
  {
    "objectID": "build-docker-image.html",
    "href": "build-docker-image.html",
    "title": "Appendix F — Build Docker image",
    "section": "",
    "text": "Build and test\n# create the Java library\nmvn clean install\n# create the docker base image\ndocker compose -f docker/build.yml build app\nThe docker compose build command has multiple --build-arg arguments to override defaults:\n\nQA_CATALOGUE_VERSION: the QA catalogue version (default: 0.7.0, current development version is 0.8.0-SNAPSHOT)\nQA_CATALOGUE_WEB_VERSION: it might be a released version such as 0.7.0, or main (default) to use the main branch, or develop to use the develop branch.\nSOLR_VERSION: the Apache Solr version you would like to use (default: 8.11.1)\nSOLR_INSTALL_SOURCE: if its value is remote docker will download it from http://archive.apache.org/. If its value is a local path points to a previously downloaded package (named as solr-${SOLR_VERSION}.zip up to version 8.x.x or solr-${SOLR_VERSION}.tgz from version 9.x.x) the process will copy it from the host to the image file. Depending on the internet connection, download might take a long time, using a previously downloaded package speeds the building process. (Note: it is not possible to specify files outside the current directory, not using symbolic links, but you can create hard links - see an example below.)\n\nUsing the current developer version:\ndocker compose -f docker/build.yml build app \\\n  --build-arg QA_CATALOGUE_VERSION=0.8.0-SNAPSHOT \\\n  --build-arg QA_CATALOGUE_WEB_VERSION=develop \\\n  --build-arg SOLR_VERSION=8.11.3\nUsing a downloaded Solr package:\n# create link temporary\nmkdir download\nln ~/Downloads/solr/solr-8.11.3.zip download/solr-8.11.3.zip\n# run docker\ndocker compose -f docker/build.yml build app \\\n  --build-arg QA_CATALOGUE_VERSION=0.8.0-SNAPSHOT \\\n  --build-arg QA_CATALOGUE_WEB_VERSION=develop \\\n  --build-arg SOLR_VERSION=8.11.3 \\\n  --build-arg SOLR_INSTALL_SOURCE=download/solr-8.11.3.zip\n# delete the temporary link\nrm -rf download\nThen start the container with environment variable IMAGE set to metadata-qa-marc and run analyses as described above.\nFor maintainers only:\nUpload to Docker Hub:\ndocker tag metadata-qa-marc:latest pkiraly/metadata-qa-marc:latest\ndocker login\ndocker push pkiraly/metadata-qa-marc:latest\nCleaning before and after:\n# stop running container\ndocker stop $(docker ps --filter name=metadata-qa-marc -q)\n# remove container\ndocker rm $(docker ps -a --filter name=metadata-qa-marc -q)\n# remove image\ndocker rmi $(docker images metadata-qa-marc -q)\n# clear build cache\ndocker builder prune -a -f",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Build Docker image</span>"
    ]
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Appendix G — Contributing",
    "section": "",
    "text": "G.1 Documentation\nQA catalogue is developed in a git repository at https://github.com/pkiraly/qa-catalogue. Bug reports and feature requests are managed in the corresponding issue tracker.\nContribution is welcome at:\nThe documentation is build with quarto. It is published at https://pkiraly.github.io/qa-catalogue/ with every change on the main branch and on the docs branch.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Contributing</span>"
    ]
  },
  {
    "objectID": "contributing.html#source-code",
    "href": "contributing.html#source-code",
    "title": "Appendix G — Contributing",
    "section": "G.2 Source code",
    "text": "G.2 Source code\nWe try to follow this git workflow:\n\nG.2.1 1st scenario: preparing the first pull request\nclone QA catalogue at https://github.com/pkiraly/qa-catalogue.git then\ngit clone https://github.com/YOUR_NAME/qa-catalogue.git qa-catalogue\ncd qa-catalogue\ncreate a new branch locally for the pull request (the name should reflect the ticket ID and title)\ngit checkout -b 123-COOL-FEATURE\nworking on the branch … then commit changes\ngit commit -am \"issue #123: explanation of changes\"\nupload the new branch to https://github.com/YOUR_NAME/qa-catalogue\ngit push -u origin 123-COOL-FEATURE\n… then create pull request at github.com/YOUR_NAME/qa-catalogue\n\n\nG.2.2 2nd scenario: preparing another pull request some month later\nregister the main QA catalogue repo\ngit remote add upstream https://github.com/pkiraly/qa-catalogue.git\n\ngit checkout main\nupdate local develop branch from https://github.com/pkiraly/qa-catalogue\ngit fetch upstream main\ngit rebase upstream/main\nupdate remote develop branch at https://github.com/YOUR_NAME/qa-catalogue\ngit push\ncreate a new branch locally for the pull request\ngit checkout -b 123-COOL-FEATURE\nwork on the branch and commit changes\ngit commit -am \"#123 explanation of changes\"\nupload the new branch to https://github.com/YOUR_NAME/qa-catalogue\ngit push -u origin 123-COOL-FEATURE\n… then create pull request at github.com/YOUR_NAME/qa-catalogue\n\n\nG.2.3 3rd scenario: synchronize your branch with main branch\ngit checkout main\nupdate local develop branch from https://github.com/pkiraly/qa-catalogue\ngit fetch upstream main\ngit rebase upstream/main\nupdate remote develop branch at https://github.com/YOUR_NAME/qa-catalogue\ngit push\nchange to the already existing feature branch\ngit checkout 123-COOL-FEATURE\nmerge changes of develop to the feature branch\ngit merge main\ncheck if there are conflicts, if there are follow the next command, otherwise skip to next block 1. fix the relevant files (including testing) 2. commit changes\ngit add &lt;fixed files&gt;\ngit commit\nupdate remote feature branch at https://github.com/YOUR_NAME/qa-catalogue\ngit push",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Contributing</span>"
    ]
  }
]